{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8cc4a28",
   "metadata": {},
   "source": [
    "# 0. 导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37032d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg') # 或 'Qt5Agg' 等\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import BaseFeaturesExtractor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Optional, Any, Union\n",
    "import cv2\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from scipy.special import softmax # For softmax function\n",
    "import os\n",
    "\n",
    "\n",
    "def load_images_to_dict(base_dir: str) -> Dict[str, List[np.ndarray]]:\n",
    "    image_datasets = {}\n",
    "    # 定义支持的图片扩展名\n",
    "    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif'}\n",
    "    \n",
    "    # 遍历基础目录下的所有子文件夹\n",
    "    for folder_name in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "        \n",
    "        # 确保是文件夹\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        \n",
    "        image_list = []\n",
    "        \n",
    "        # 遍历文件夹中的文件\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # 检查文件扩展名\n",
    "            ext = os.path.splitext(filename)[1].lower()\n",
    "            if ext not in valid_extensions:\n",
    "                continue  # 跳过非图片文件\n",
    "            \n",
    "            try:\n",
    "                # 用PIL打开图片并转换为RGB（避免Alpha通道问题）\n",
    "                with Image.open(file_path) as img:\n",
    "                    img = img.convert('RGB')  # 统一转换为RGB三通道\n",
    "                    image_list.append(np.array(img))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # 将文件夹名作为key，图片数组列表作为value\n",
    "        image_datasets[folder_name] = image_list\n",
    "    \n",
    "    return image_datasets\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_list: List[np.ndarray], transform: Any = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_list (List[np.ndarray]): List of images as NumPy arrays.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.images = image_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        image_np = self.images[idx]\n",
    "\n",
    "        # Ensure image is in uint8 format for PIL conversion if necessary,\n",
    "        # or if ToTensor is directly applied to numpy array.\n",
    "        if image_np.dtype != np.uint8:\n",
    "            # This can happen if images were, e.g., float32 from some processing.\n",
    "            # Assuming they were originally 0-255 range if they are not uint8.\n",
    "            # If they are already 0-1 float, this needs adjustment.\n",
    "            # For typical image data, astype(np.uint8) is safe.\n",
    "            image_np = image_np.astype(np.uint8)\n",
    "\n",
    "        # transforms.ToTensor() can handle HWC uint8 NumPy arrays directly.\n",
    "        # It will convert to CHW FloatTensor and scale to [0.0, 1.0].\n",
    "        # If you prefer to use PIL Image explicitly:\n",
    "        # image_pil = Image.fromarray(image_np)\n",
    "        # if self.transform:\n",
    "        #     image_tensor = self.transform(image_pil)\n",
    "        # else: # Fallback if no transform, though ToTensor is crucial\n",
    "        #     image_tensor = transforms.ToTensor()(image_pil)\n",
    "\n",
    "        # Direct application to NumPy array (preferred if array is HWC)\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_np)\n",
    "        else:\n",
    "            # Default to ToTensor if no specific transform is given,\n",
    "            # as it's essential for PyTorch models.\n",
    "            image_tensor = transforms.ToTensor()(image_np)\n",
    "\n",
    "        return image_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f63e2",
   "metadata": {},
   "source": [
    "# 1. Survival Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a738045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# 环境配置\n",
    "# ========================\n",
    "MAP_SIZE = 50\n",
    "PREDATOR_COUNT = 3\n",
    "MAX_FOOD = 250\n",
    "CLUSTER_RADIUS = 3\n",
    "PIXEL_TYPES = {\n",
    "    0: 'environment',\n",
    "    1: 'predator',\n",
    "    2: 'food',\n",
    "    3: 'agent'\n",
    "}\n",
    "REWARDS = {\n",
    "    'predator': -50,  # 调整为文档要求的-50\n",
    "    'food': +10,      # 调整为文档要求的+10\n",
    "    'environment': -1\n",
    "}\n",
    "NUM_VIEWS = 4  # Front, Left, Right, Back\n",
    "INITIAL_SCORE = 100  # 添加初始分数\n",
    "\n",
    "# ========================\n",
    "# 环境实现\n",
    "# ========================\n",
    "class SurvivalGameEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': 4}\n",
    "\n",
    "    def __init__(self, image_datasets: Dict[str, List[np.ndarray]]):\n",
    "        super(SurvivalGameEnv, self).__init__()\n",
    "\n",
    "        self.image_datasets = image_datasets\n",
    "        # This transform is for external use if needed, model handles its own\n",
    "        self.vis_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # These will be converted to probabilities for Escape, Eat, Wander inside step()\n",
    "        # stable_baselines3 requires finite bounds for Box action space.\n",
    "        # These outputs will be treated as logits for 3 intents.\n",
    "        # A large range like [-100, 100] should be sufficient.\n",
    "        self.action_space = spaces.Box(low=-100.0, high=100.0, shape=(3,), dtype=np.float32)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255,\n",
    "            shape=(NUM_VIEWS, 100, 100, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        self.current_map_image = None  # For rendering\n",
    "        self.fig = None\n",
    "        self.ax = None\n",
    "        self.current_map_image_artist = None # 将存储imshow返回的artist对象\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "        super().reset(seed=seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # 初始化地图\n",
    "        self.map = np.zeros((MAP_SIZE, MAP_SIZE), dtype=np.uint8)\n",
    "        self.predators = []\n",
    "        self.foods = {}\n",
    "        \n",
    "        # 放置捕食者\n",
    "        for _ in range(PREDATOR_COUNT):\n",
    "            pos = self._random_edge_position()\n",
    "            while self.map[pos] != 0:  # 确保位置为空\n",
    "                pos = self._random_edge_position()\n",
    "            self.map[pos] = 1\n",
    "            self.predators.append({\n",
    "                'pos': pos,\n",
    "                'img_idx': np.random.randint(len(self.image_datasets['predator']))\n",
    "            })\n",
    "        \n",
    "        # 集群式生成食物\n",
    "        num_clusters = max(1, MAX_FOOD // 10)  # 每10个食物一个集群\n",
    "        cluster_centers = [self._random_position() for _ in range(num_clusters)]\n",
    "        \n",
    "        for center in cluster_centers:\n",
    "            # 使用高斯分布生成集群\n",
    "            cluster_size = min(MAX_FOOD - len(self.foods), MAX_FOOD // num_clusters)\n",
    "            for _ in range(cluster_size):\n",
    "                # 在集群半径内生成随机偏移\n",
    "                dx = int(np.random.normal(0, CLUSTER_RADIUS / 2))\n",
    "                dy = int(np.random.normal(0, CLUSTER_RADIUS / 2))\n",
    "                pos = (\n",
    "                    np.clip(center[0] + dx, 0, MAP_SIZE - 1),\n",
    "                    np.clip(center[1] + dy, 0, MAP_SIZE - 1)\n",
    "                )\n",
    "                \n",
    "                if self._is_valid_position(pos) and self.map[pos] == 0:\n",
    "                    self.map[pos] = 2\n",
    "                    self.foods[pos] = {\n",
    "                        'img_idx': np.random.randint(len(self.image_datasets['food'])),\n",
    "                        'consumed': False\n",
    "                    }\n",
    "                    if len(self.foods) >= MAX_FOOD:\n",
    "                        break\n",
    "            if len(self.foods) >= MAX_FOOD:\n",
    "                break\n",
    "\n",
    "        # 放置代理\n",
    "        self.agent_pos = self._random_position()\n",
    "        while self.map[self.agent_pos] != 0:\n",
    "            self.agent_pos = self._random_position()\n",
    "        \n",
    "        # 初始化代理方向和分数\n",
    "        self.agent_direction = 0  # 0:上, 1:左, 2:右, 3:下\n",
    "        self.score = INITIAL_SCORE  # 添加分数系统\n",
    "        \n",
    "        self.prev_agent_pos = self.agent_pos\n",
    "        self.steps = 0\n",
    "        self.terminated = False\n",
    "        self.truncated = False\n",
    "\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, raw_actions: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:\n",
    "        if not (isinstance(raw_actions, np.ndarray) and raw_actions.shape == (3,)):\n",
    "            raise ValueError(\n",
    "                f\"Invalid action type or shape passed to env.step(). \"\n",
    "                f\"Expected 3-element np.ndarray (logits/preferences), got {type(raw_actions)} with value {raw_actions}\"\n",
    "            )\n",
    "        # Convert raw_actions (logits/preferences) to probabilities for the 3 high-level intents\n",
    "        # using softmax.\n",
    "        # raw_actions can be thought of as the direct output of the policy network's last layer\n",
    "        # for the 3 high-level intents before any sampling.\n",
    "        intent_probs = softmax(raw_actions).astype(np.float32)\n",
    "\n",
    "        # 1. 计算移动概率分布 (using the new intent_probs)\n",
    "        move_probs_dist = self._calculate_movement_distribution(intent_probs) # This method already expects probabilities\n",
    "\n",
    "        # 2. 选择移动方向\n",
    "        chosen_direction_idx = np.random.choice(NUM_VIEWS, p=move_probs_dist)\n",
    "\n",
    "        # 3. 移动代理并更新方向\n",
    "        self.prev_agent_pos = self.agent_pos\n",
    "        self.agent_pos = self._move_in_direction(self.agent_pos, chosen_direction_idx)\n",
    "        self.agent_direction = chosen_direction_idx\n",
    "\n",
    "        # 4. 处理交互和奖励\n",
    "        reward = 0\n",
    "        collided_entity_type = self.map[self.agent_pos]\n",
    "\n",
    "        if collided_entity_type == 1:\n",
    "            reward += REWARDS['predator']\n",
    "            #self.terminated = True\n",
    "        elif collided_entity_type == 2:\n",
    "            if self.agent_pos in self.foods:\n",
    "                reward += REWARDS['food']\n",
    "                del self.foods[self.agent_pos]\n",
    "                self.map[self.agent_pos] = 0\n",
    "        elif collided_entity_type == 0:\n",
    "            reward += REWARDS['environment']\n",
    "        else:\n",
    "            reward += REWARDS['environment']\n",
    "\n",
    "        self.score += reward\n",
    "        if self.score < 1:\n",
    "            self.terminated = True\n",
    "\n",
    "        self._move_predators()\n",
    "\n",
    "        if not self.terminated:\n",
    "            for pred in self.predators:\n",
    "                if pred['pos'] == self.agent_pos:\n",
    "                    # If caught by predator moving into agent's square\n",
    "                    # Ensure this penalty is applied correctly, potentially overwriting previous reward for this step\n",
    "                    # Or add a large negative value. Let's assume it sets the reward for this step to predator penalty.\n",
    "                    current_step_reward_value = REWARDS['predator']\n",
    "                    self.score += (current_step_reward_value - reward) # Adjust score based on the new reward for this step\n",
    "                    reward = current_step_reward_value\n",
    "\n",
    "                    if self.score < 1:\n",
    "                        self.score = 0\n",
    "                        self.terminated = True\n",
    "                    break\n",
    "        \n",
    "        self.steps += 1\n",
    "        if self.steps >= 1000:\n",
    "            self.truncated = True\n",
    "\n",
    "        return self._get_observation(), float(reward), self.terminated, self.truncated, {}\n",
    "\n",
    "    def _get_direction_vectors(self) -> List[Tuple[int, int]]:\n",
    "        \"\"\"获取方向向量：前、左、右、后（相对于当前方向）\"\"\"\n",
    "        # 绝对方向：0:上, 1:左, 2:右, 3:下\n",
    "        # 根据代理当前方向计算相对方向\n",
    "        if self.agent_direction == 0:  # 面向\"上\"\n",
    "            return [(-1, 0), (0, -1), (0, 1), (1, 0)]  # 前、左、右、后\n",
    "        elif self.agent_direction == 1:  # 面向\"左\"\n",
    "            return [(0, -1), (1, 0), (-1, 0), (0, 1)]  # 前、左、右、后\n",
    "        elif self.agent_direction == 2:  # 面向\"右\"\n",
    "            return [(0, 1), (-1, 0), (1, 0), (0, -1)]  # 前、左、右、后\n",
    "        else:  # 面向\"下\"\n",
    "            return [(1, 0), (0, 1), (0, -1), (-1, 0)]  # 前、左、右、后\n",
    "\n",
    "    def _get_observation(self) -> np.ndarray:\n",
    "        \"\"\"获取代理四个方向的观察图像（基于当前方向）\"\"\"\n",
    "        observations = []\n",
    "        direction_vectors = self._get_direction_vectors()\n",
    "        \n",
    "        # Determine a default image shape from the observation space if a category has no images\n",
    "        # obs_space shape is (NUM_VIEWS, H, W, C)\n",
    "        # Individual image shape is (H, W, C)\n",
    "        h_obs, w_obs, c_obs = self.observation_space.shape[1], self.observation_space.shape[2], self.observation_space.shape[3]\n",
    "        default_image_for_category = np.zeros((h_obs, w_obs, c_obs), dtype=np.uint8)\n",
    "        \n",
    "        for dr, dc in direction_vectors:\n",
    "            target_pos = (\n",
    "                (self.agent_pos[0] + dr) % MAP_SIZE,\n",
    "                (self.agent_pos[1] + dc) % MAP_SIZE\n",
    "            )\n",
    "            entity_type = self.map[target_pos]\n",
    "            \n",
    "            img_array = None # Initialize to ensure it gets assigned\n",
    "            \n",
    "            if entity_type == 1:  # 捕食者\n",
    "                predator_images = self.image_datasets.get('predator', []) # Get list, default to empty\n",
    "                specific_pred_img = None\n",
    "                \n",
    "                # Try to find the specific predator for this position to use its assigned image\n",
    "                if predator_images: # Only proceed if there are predator images\n",
    "                    for p in self.predators:\n",
    "                        if p['pos'] == target_pos:\n",
    "                            img_idx = p['img_idx']\n",
    "                            if 0 <= img_idx < len(predator_images):\n",
    "                                specific_pred_img = predator_images[img_idx]\n",
    "                            else:\n",
    "                                # Log if a specific predator has an invalid img_idx\n",
    "                                print(f\"Warning: Predator at {target_pos} has invalid img_idx {img_idx}. Using random predator image.\")\n",
    "                            break # Found the predator entry in self.predators\n",
    "                \n",
    "                if specific_pred_img is not None:\n",
    "                    img_array = specific_pred_img\n",
    "                elif predator_images: # Fallback to a random predator image if specific one not found/invalid\n",
    "                    img_array = random.choice(predator_images)\n",
    "                else: # No predator images available at all\n",
    "                    img_array = default_image_for_category\n",
    "                    # Optional: print(\"Warning: 'predator' image dataset is empty. Using default image.\")\n",
    "\n",
    "            elif entity_type == 2:  # 食物\n",
    "                food_images = self.image_datasets.get('food', [])\n",
    "                food_details = self.foods.get(target_pos)\n",
    "\n",
    "                if food_details and food_images: # If details for this food exist and there are food images\n",
    "                    img_idx = food_details.get('img_idx', -1) # Use -1 to indicate invalid if key missing\n",
    "                    if 0 <= img_idx < len(food_images):\n",
    "                        img_array = food_images[img_idx]\n",
    "                    else: # img_idx is out of bounds or was missing, pick a random one\n",
    "                        # print(f\"Warning: Food at {target_pos} has invalid/missing img_idx. Using random food image.\")\n",
    "                        img_array = random.choice(food_images)\n",
    "                elif food_images: # Food on map, but no details in self.foods (consistency issue) or no food_details. Pick random.\n",
    "                    # print(f\"Warning: Food details not found for {target_pos} or img_idx issue. Using random food image.\")\n",
    "                    img_array = random.choice(food_images)\n",
    "                else: # No food images available\n",
    "                    img_array = default_image_for_category\n",
    "                    # Optional: print(\"Warning: 'food' image dataset is empty. Using default image.\")\n",
    "            \n",
    "            else:  # 环境 (entity_type == 0 or any other type)\n",
    "                environment_images = self.image_datasets.get('environment', [])\n",
    "                if environment_images:\n",
    "                    img_array = random.choice(environment_images)\n",
    "                else: # No environment images available\n",
    "                    img_array = default_image_for_category\n",
    "                    # Optional: print(\"Warning: 'environment' image dataset is empty. Using default image.\")\n",
    "            \n",
    "            # This check should ideally not be needed if logic above is exhaustive\n",
    "            if img_array is None:\n",
    "                print(f\"Critical Warning: img_array ended up as None for {target_pos}, entity_type {entity_type}. Using default.\")\n",
    "                img_array = default_image_for_category\n",
    "\n",
    "            observations.append(img_array)\n",
    "        \n",
    "        return np.array(observations, dtype=np.uint8)\n",
    "\n",
    "    def _calculate_movement_distribution(self, action_probs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"计算移动概率分布（基于相对方向）\"\"\"\n",
    "        # 获取周围实体\n",
    "        surrounding_entities = []\n",
    "        direction_vectors = self._get_direction_vectors()\n",
    "        \n",
    "        for dr, dc in direction_vectors:\n",
    "            pos = (\n",
    "                (self.agent_pos[0] + dr) % MAP_SIZE,\n",
    "                (self.agent_pos[1] + dc) % MAP_SIZE\n",
    "            )\n",
    "            surrounding_entities.append(self.map[pos])\n",
    "        \n",
    "        move_dist = np.zeros(NUM_VIEWS)\n",
    "        \n",
    "        # Escape逻辑：远离感知到的捕食者（向相反方向移动）\n",
    "        predator_sensed = False\n",
    "        for i, entity in enumerate(surrounding_entities):\n",
    "            if entity == 1:  # 捕食者\n",
    "                # 相反方向是索引3-i（前<->后，左<->右）\n",
    "                opposite_idx = 3 - i\n",
    "                move_dist[opposite_idx] += action_probs[0]\n",
    "                predator_sensed = True\n",
    "        \n",
    "        if not predator_sensed:\n",
    "            move_dist += action_probs[0] / NUM_VIEWS\n",
    "        \n",
    "        # Eat逻辑：朝向感知到的食物\n",
    "        food_sensed = False\n",
    "        for i, entity in enumerate(surrounding_entities):\n",
    "            if entity == 2:  # 食物\n",
    "                move_dist[i] += action_probs[1]\n",
    "                food_sensed = True\n",
    "        \n",
    "        if not food_sensed:\n",
    "            move_dist += action_probs[1] / NUM_VIEWS\n",
    "        \n",
    "        # Wander逻辑：随机移动\n",
    "        move_dist += action_probs[2] / NUM_VIEWS\n",
    "        \n",
    "        # 归一化\n",
    "        total = np.sum(move_dist)\n",
    "        if total > 0:\n",
    "            move_dist /= total\n",
    "        else:\n",
    "            move_dist = np.ones(NUM_VIEWS) / NUM_VIEWS\n",
    "        \n",
    "        return move_dist\n",
    "\n",
    "    def _move_in_direction(self, current_pos: Tuple[int, int], direction_idx: int) -> Tuple[int, int]:\n",
    "        \"\"\"在指定方向上移动位置（基于相对方向）\"\"\"\n",
    "        dr, dc = self._get_direction_vectors()[direction_idx]\n",
    "        new_r = (current_pos[0] + dr) % MAP_SIZE\n",
    "        new_c = (current_pos[1] + dc) % MAP_SIZE\n",
    "        return new_r, new_c\n",
    "\n",
    "    def _move_predators(self):\n",
    "        \"\"\"改进的捕食者移动逻辑，解决移动冲突且禁止移动到食物位置\"\"\"\n",
    "        # 清除所有捕食者位置标记（临时设为环境）\n",
    "        for pred in self.predators:\n",
    "            self.map[pred['pos']] = 0\n",
    "        \n",
    "        moved_positions = set()  # 记录所有计划移动到的位置\n",
    "        new_predators = []       # 存储移动后的捕食者\n",
    "        move_vectors = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # 上、下、左、右\n",
    "        \n",
    "        # 随机顺序处理捕食者，确保公平性\n",
    "        random.shuffle(self.predators)\n",
    "        \n",
    "        for pred in self.predators:\n",
    "            current_pos = pred['pos']\n",
    "            moved = False\n",
    "            \n",
    "            # 尝试所有可能的移动方向（随机顺序）\n",
    "            random.shuffle(move_vectors)\n",
    "            \n",
    "            for dr, dc in move_vectors:\n",
    "                new_pos = (current_pos[0] + dr, current_pos[1] + dc)\n",
    "                \n",
    "                # 检查是否移出边界\n",
    "                if not (0 <= new_pos[0] < MAP_SIZE and 0 <= new_pos[1] < MAP_SIZE):\n",
    "                    # 在边缘生成新捕食者\n",
    "                    edge_pos = self._random_edge_position()\n",
    "                    \n",
    "                    # 确保新位置有效且未被占用\n",
    "                    if self.map[edge_pos] == 0 and edge_pos not in moved_positions:\n",
    "                        new_pred = {\n",
    "                            'pos': edge_pos,\n",
    "                            'img_idx': np.random.randint(len(self.image_datasets['predator']))\n",
    "                        }\n",
    "                        self.map[edge_pos] = 1\n",
    "                        moved_positions.add(edge_pos)\n",
    "                        new_predators.append(new_pred)\n",
    "                        moved = True\n",
    "                        break\n",
    "                    # 如果位置无效，继续尝试其他方向\n",
    "                    continue\n",
    "                \n",
    "                # 检查目标位置是否有效（只能是环境或代理，不能是食物或其他捕食者）\n",
    "                if self.map[new_pos] != 1 and self.map[new_pos] != 2 and new_pos not in moved_positions:\n",
    "                    # 检查是否与食物或其他捕食者冲突\n",
    "                    pred['pos'] = new_pos\n",
    "                    self.map[new_pos] = 1\n",
    "                    moved_positions.add(new_pos)\n",
    "                    new_predators.append(pred)\n",
    "                    moved = True\n",
    "                    break\n",
    "            \n",
    "            # 如果无法移动，留在原位（检查位置是否仍可用）\n",
    "            if not moved:\n",
    "                if current_pos not in moved_positions and self.map[current_pos] != 2:  # 不是食物\n",
    "                    self.map[current_pos] = 1\n",
    "                    moved_positions.add(current_pos)\n",
    "                    new_predators.append(pred)\n",
    "                else:\n",
    "                    # 当前位置已被占用，在随机位置生成新捕食者\n",
    "                    new_pos = self._random_position()\n",
    "                    if self.map[new_pos] == 0 and new_pos not in moved_positions:\n",
    "                        new_pred = {\n",
    "                            'pos': new_pos,\n",
    "                            'img_idx': np.random.randint(len(self.image_datasets['predator']))\n",
    "                        }\n",
    "                        self.map[new_pos] = 1\n",
    "                        moved_positions.add(new_pos)\n",
    "                        new_predators.append(new_pred)\n",
    "        \n",
    "        self.predators = new_predators\n",
    "\n",
    "    def _random_position(self) -> Tuple[int, int]:\n",
    "        \"\"\"随机位置（地图内）\"\"\"\n",
    "        return (np.random.randint(0, MAP_SIZE), np.random.randint(0, MAP_SIZE))\n",
    "\n",
    "    def _random_edge_position(self) -> Tuple[int, int]:\n",
    "        \"\"\"随机边缘位置\"\"\"\n",
    "        edge = np.random.choice(['top', 'bottom', 'left', 'right'])\n",
    "        if edge == 'top': \n",
    "            return (0, np.random.randint(0, MAP_SIZE))\n",
    "        if edge == 'bottom': \n",
    "            return (MAP_SIZE - 1, np.random.randint(0, MAP_SIZE))\n",
    "        if edge == 'left': \n",
    "            return (np.random.randint(0, MAP_SIZE), 0)\n",
    "        return (np.random.randint(0, MAP_SIZE), MAP_SIZE - 1)  # right\n",
    "\n",
    "    def _is_valid_position(self, pos: Tuple[int, int]) -> bool:\n",
    "        \"\"\"检查位置是否有效\"\"\"\n",
    "        return 0 <= pos[0] < MAP_SIZE and 0 <= pos[1] < MAP_SIZE\n",
    "\n",
    "    def render(self, mode='human'): # 添加 mode 参数以符合gym规范\n",
    "        if mode != 'human':\n",
    "            return\n",
    "\n",
    "        # 检查存储的figure是否仍然有效和打开\n",
    "        # 如果 self.fig 是 None (首次调用) 或 figure编号不存在 (窗口已关闭)\n",
    "        if self.fig is None or not plt.fignum_exists(self.fig.number):\n",
    "            # 如果存在一个其窗口已关闭的旧figure对象，清理它\n",
    "            if self.fig is not None:\n",
    "                plt.close(self.fig) # 关闭figure对象本身\n",
    "\n",
    "            # 创建新的figure和axes\n",
    "            self.fig, self.ax = plt.subplots(figsize=(8, 8)) # 可调整大小\n",
    "            \n",
    "            # 为imshow artist准备初始地图数据\n",
    "            # self.map 应该在首次调用render之前由reset()初始化\n",
    "            initial_display_map = self.map.copy()\n",
    "            # self.agent_pos 也应该由reset()初始化\n",
    "            # 确保属性存在，避免在reset()之前调用render()出错\n",
    "            if hasattr(self, 'agent_pos') and self.agent_pos is not None:\n",
    "                initial_display_map[self.agent_pos[0], self.agent_pos[1]] = 3 # 标记智能体\n",
    "\n",
    "            # 创建图像艺术家 (image artist)\n",
    "            self.current_map_image_artist = self.ax.imshow(\n",
    "                initial_display_map,\n",
    "                cmap='viridis',\n",
    "                vmin=0,\n",
    "                vmax=len(PIXEL_TYPES) - 1 \n",
    "            )\n",
    "            \n",
    "            # 为新绘图添加颜色条、标题并关闭坐标轴显示\n",
    "            self.fig.colorbar(self.current_map_image_artist, ax=self.ax, label='Entity Types')\n",
    "            self.ax.set_title(\"Survival Game Environment\") # 初始标题\n",
    "            self.ax.axis('off')\n",
    "            \n",
    "            # 确保matplotlib处于交互模式\n",
    "            if not plt.isinteractive():\n",
    "                plt.ion()\n",
    "            \n",
    "            # 显示图形。对于某些后端，plt.show(block=False)或仅让事件循环运行就足够了。\n",
    "            # self.fig.show() 可能更适合某些情况，或者直接依赖 plt.pause() 来刷新。\n",
    "            # 为了简单起见，首次显示依赖于Gym环境的典型用法，即plt.pause()会处理。\n",
    "            # 如果在Jupyter中，%matplotlib widget会自动处理显示。\n",
    "\n",
    "        # --- 更新现有绘图以反映当前游戏状态 ---\n",
    "        current_display_map = self.map.copy()\n",
    "        if hasattr(self, 'agent_pos') and self.agent_pos is not None:\n",
    "            current_display_map[self.agent_pos[0], self.agent_pos[1]] = 3 # 标记智能体当前位置\n",
    "\n",
    "        # 更新图像艺术家的数据\n",
    "        if self.current_map_image_artist: # 确保artist存在\n",
    "            self.current_map_image_artist.set_data(current_display_map)\n",
    "        \n",
    "        # 更新标题\n",
    "        title_str = \"Survival Game\"\n",
    "        if hasattr(self, 'steps'): title_str += f\" Step: {self.steps}\"\n",
    "        if hasattr(self, 'agent_pos'): title_str += f\", Agent: {self.agent_pos}\"\n",
    "        if hasattr(self, 'score'): title_str += f\", Score: {self.score:.1f}\"\n",
    "        \n",
    "        if self.ax: # 确保ax存在\n",
    "            self.ax.set_title(title_str)\n",
    "        \n",
    "        # 重绘画布\n",
    "        if self.fig: # 确保fig存在\n",
    "            self.fig.canvas.draw_idle()\n",
    "            # self.fig.canvas.flush_events() # 对于某些后端可能需要\n",
    "        \n",
    "        # 暂停以允许绘图更新并可见\n",
    "        plt.pause(0.05) # 较短的暂停时间，使动画更平滑\n",
    "\n",
    "    def close(self):\n",
    "        if self.fig is not None:\n",
    "            plt.close(self.fig) # 关闭图形窗口和figure对象\n",
    "            self.fig = None\n",
    "            self.ax = None\n",
    "            self.current_map_image_artist = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8f5316",
   "metadata": {},
   "source": [
    "# 2. 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3afaa4",
   "metadata": {},
   "source": [
    "## 2.1 模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de989b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# Model Architecture (Wrappers for Perception)\n",
    "# ========================\n",
    "class PerceptionModule(nn.Module):\n",
    "    def __init__(self, pretrained_convnext=True):\n",
    "        super(PerceptionModule, self).__init__()\n",
    "        self.convnext = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.DEFAULT if pretrained_convnext else None)\n",
    "        self.convnext.classifier = nn.Identity() # Remove original classifier\n",
    "        \n",
    "        self.do_normalize = pretrained_convnext\n",
    "        if self.do_normalize:\n",
    "            self.normalize_transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x is (B, N_VIEWS, C, H, W), scaled [0,1]\n",
    "        \n",
    "        # Hooks for visualization (GradCAM needs these on a specific layer, see ModelVisualizer)\n",
    "        # For general input/output grads if needed:\n",
    "        # if x.requires_grad:\n",
    "        #     x.register_hook(lambda grad: setattr(self, 'input_gradients', grad))\n",
    "        \n",
    "        batch_size, num_views, C, H, W = x.size()\n",
    "        x_input_to_convnext = x.view(batch_size * num_views, C, H, W)\n",
    "\n",
    "        if self.do_normalize:\n",
    "            x_input_to_convnext = self.normalize_transform(x_input_to_convnext)\n",
    "        \n",
    "        # Register hook for GradCAM on the output of the target layer if not done externally\n",
    "        # features = self.convnext(x_input_to_convnext)\n",
    "        # For GradCAM, typically want features from a specific conv layer, not the final output of convnext here.\n",
    "        # The ModelVisualizer will handle hooking the specific internal layer.\n",
    "        \n",
    "        raw_features = self.convnext(x_input_to_convnext) # Output: (B*N_VIEWS, feature_dim e.g. 768)\n",
    "        return raw_features.view(batch_size, num_views, -1) # (B, N_VIEWS, feature_dim)\n",
    "\n",
    "class PretrainedEncoderWrapper(nn.Module):\n",
    "    def __init__(self, encoder_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder_model # This is a ConvNeXt model (autoencoder.encoder)\n",
    "        # AE encoder was trained on [0,1] images, so no ImageNet normalization here.\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x is (batch_size, num_views, C, H, W), already scaled to [0,1]\n",
    "        batch_size, num_views, C, H, W = x.size()\n",
    "        x_reshaped = x.view(batch_size * num_views, C, H, W)\n",
    "        \n",
    "        features = self.encoder(x_reshaped) # encoder is ConvNeXt, outputs (B*N_VIEWS, 768)\n",
    "        return features.view(batch_size, num_views, -1)\n",
    "\n",
    "# Decision module as defined in doc (used by ModelVisualizer, SB3 PPO has its own MLP head)\n",
    "class StandaloneDecisionModule(nn.Module):\n",
    "    def __init__(self, input_dim: int = NUM_VIEWS * 768, hidden_dims: List[int] = [512, 256]):\n",
    "        super(StandaloneDecisionModule, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, 3)) # Output 3 action probabilities\n",
    "        layers.append(nn.Softmax(dim=-1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x is (B, N_VIEWS * feature_dim)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# 自编码器\n",
    "# ========================\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.DEFAULT) # Or None for from scratch\n",
    "        self.encoder.classifier = nn.Identity() # Output of encoder is (B, 768)\n",
    "\n",
    "        # Decoder: (B, 768, 1, 1) -> (B, 3, 100, 100)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(768, 512, kernel_size=5, stride=1, padding=0), # (B, 512, 5, 5)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=5, padding=0), # (B, 256, 25, 25)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # (B, 128, 50, 50)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # (B, 64, 100, 100)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=3, stride=1, padding=1),   # (B, 3, 100, 100)\n",
    "            nn.Sigmoid() # Output images in [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # x is (B, 3, H, W)\n",
    "        encoded_flat = self.encoder(x) # (B, 768)\n",
    "        encoded_reshaped = encoded_flat.view(-1, 768, 1, 1) # Reshape for ConvTranspose\n",
    "        decoded = self.decoder(encoded_reshaped)\n",
    "        return decoded\n",
    "\n",
    "# ========================\n",
    "# 特征提取器 for Stable Baselines3\n",
    "# ========================\n",
    "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, perception_module: nn.Module):\n",
    "        # features_dim is N_VIEWS * perception_module_output_dim_per_view\n",
    "        # Assuming perception_module outputs 768 features per view\n",
    "        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim=NUM_VIEWS * 768)\n",
    "        self.perception = perception_module # This is PerceptionModule or PretrainedEncoderWrapper\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        # Input: (batch, NUM_VIEWS, H, W, C) uint8\n",
    "        # Permute to (batch, NUM_VIEWS, C, H, W) and scale to [0,1]\n",
    "        observations_processed = observations.permute(0, 1, 4, 2, 3).float() / 255.0\n",
    "        \n",
    "        # self.perception module (PerceptionModule or PretrainedEncoderWrapper)\n",
    "        # expects (B, N_VIEWS, C, H, W) scaled [0,1]\n",
    "        # and returns (B, N_VIEWS, feature_dim_per_view)\n",
    "        features_per_view = self.perception(observations_processed)\n",
    "        \n",
    "        # Flatten features from all views: (B, N_VIEWS * feature_dim_per_view)\n",
    "        return features_per_view.reshape(observations.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c1a2e",
   "metadata": {},
   "source": [
    "## 2.2 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "533eb225",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# 训练方法\n",
    "# ========================\n",
    "def fitness_training(env: gym.Env, total_timesteps: int = 10000):\n",
    "    # PerceptionModule handles its own normalization if pretrained\n",
    "    perception_fitness = PerceptionModule(pretrained_convnext=True)\n",
    "    # Move perception_fitness to the device PPO will use.\n",
    "    # If PPO is on CPU, perception module should also be on CPU.\n",
    "    # If PPO is on GPU, perception module should also be on GPU.\n",
    "    # Since we're likely choosing 'cpu' for PPO, let's ensure it.\n",
    "    # However, CustomFeatureExtractor in SB3 handles moving its submodules\n",
    "    # to the PPO agent's device automatically. So, manual moving here isn't strictly necessary\n",
    "    # but doesn't hurt for clarity if you want to be explicit.\n",
    "    # device_to_use = \"cpu\" # or \"cuda\" if you decide to try GPU despite warning\n",
    "    # perception_fitness.to(device_to_use)\n",
    "\n",
    "    policy_kwargs = dict(\n",
    "        features_extractor_class=CustomFeatureExtractor,\n",
    "        features_extractor_kwargs=dict(perception_module=perception_fitness),\n",
    "        # Corrected net_arch: remove the outer list\n",
    "        net_arch=dict(pi=[256, 128], vf=[256, 128]),\n",
    "    )\n",
    "\n",
    "    # Corrected device: explicitly set to \"cpu\" to address the warning\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, device=\"cpu\")\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    print(f\"Total timesteps = {total_timesteps}\")\n",
    "    return model\n",
    "\n",
    "def truth_training(env: gym.Env, ae_path: str, total_timesteps: int = 10000):\n",
    "    autoencoder = Autoencoder()\n",
    "    # Determine device before loading state_dict if model might move\n",
    "    # For simplicity, let's assume ae_encoder will be moved to PPO's device\n",
    "    # by the CustomFeatureExtractor.\n",
    "    autoencoder.load_state_dict(torch.load(ae_path)) # load_state_dict typically loads to CPU by default\n",
    "    ae_encoder = autoencoder.encoder\n",
    "\n",
    "    for param in ae_encoder.parameters(): # Freeze encoder parameters\n",
    "        param.requires_grad = False\n",
    "\n",
    "    perception_truth = PretrainedEncoderWrapper(ae_encoder)\n",
    "    # Similar to fitness_training, CustomFeatureExtractor will handle device placement\n",
    "    # device_to_use = \"cpu\" # or \"cuda\"\n",
    "    # perception_truth.to(device_to_use)\n",
    "\n",
    "    policy_kwargs = dict(\n",
    "        features_extractor_class=CustomFeatureExtractor,\n",
    "        features_extractor_kwargs=dict(perception_module=perception_truth),\n",
    "        # Corrected net_arch: remove the outer list\n",
    "        net_arch=dict(pi=[256, 128], vf=[256, 128])\n",
    "    )\n",
    "\n",
    "    # Corrected device: explicitly set to \"cpu\"\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, device=\"cpu\")\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    print(f\"Total timesteps = {total_timesteps}\")\n",
    "    return model\n",
    "\n",
    "def train_autoencoder(image_datasets: Dict[str, List[np.ndarray]],\n",
    "                      epochs: int = 10,\n",
    "                      batch_size: int = 32,\n",
    "                      ae_save_path: str = \"autoencoder.pth\",\n",
    "                      num_workers: int = 0 # For DataLoader: 0 means data loaded in main process\n",
    "                     ) -> Autoencoder: # Assuming Autoencoder class is defined\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Autoencoder().to(device) # Ensure Autoencoder class is defined\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Combine all images for training from the input dictionary\n",
    "    all_images_np = [img for cat_imgs in image_datasets.values() for img in cat_imgs]\n",
    "\n",
    "    if not all_images_np:\n",
    "        print(\"No images found to train the autoencoder. Aborting.\")\n",
    "        return model # Or raise an error\n",
    "\n",
    "    # Define the transformation - ToTensor is crucial\n",
    "    # It converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255]\n",
    "    # to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Create the custom dataset\n",
    "    custom_dataset = CustomImageDataset(image_list=all_images_np, transform=transform)\n",
    "\n",
    "    # Create the DataLoader\n",
    "    # The DataLoader will now handle batching and shuffling efficiently.\n",
    "    # Images are loaded and transformed on-the-fly (or by worker processes if num_workers > 0)\n",
    "    # and only a batch at a time is moved to the GPU.\n",
    "    dataloader = DataLoader(custom_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=num_workers, # Adjust based on your system\n",
    "                            pin_memory=True if device.type == 'cuda' else False) # Speeds up CPU to GPU transfer\n",
    "\n",
    "    print(f\"Training Autoencoder with {len(custom_dataset)} images on {device}...\")\n",
    "    print(f\"Number of batches per epoch: {len(dataloader)}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, inputs in enumerate(dataloader): # inputs are (B, C, H, W)\n",
    "            # Move the batch of inputs to the designated device\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs) # Autoencoder reconstructs the input\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if (i + 1) % (max(1, len(dataloader) // 10)) == 0: # Print progress 10 times per epoch\n",
    "                print(f\"  Epoch {epoch+1}/{epochs}, Batch {i+1}/{len(dataloader)}, \"\n",
    "                      f\"Current Avg Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "        epoch_loss = running_loss / len(custom_dataset)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} finished. Average Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), ae_save_path)\n",
    "    print(f\"Autoencoder trained and saved to {ae_save_path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176dafac",
   "metadata": {},
   "source": [
    "# 3. 评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf844a",
   "metadata": {},
   "source": [
    "## 3.1 Survival Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a22c07c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# 生存评估函数\n",
    "# ========================\n",
    "\n",
    "# ... (evaluate_survival) ...\n",
    "def evaluate_survival(model: PPO, env: gym.Env, n_episodes: int = 10) -> float:\n",
    "    total_steps = 0\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        episode_steps = 0\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            # For Box space, model.predict returns the continuous action vector\n",
    "            # These are the \"raw_actions\" (logits/preferences) our env.step now expects\n",
    "            action_vector, _ = model.predict(obs, deterministic=True) # Use deterministic for evaluation\n",
    "\n",
    "            # Environment interaction (env.step now handles softmax internally)\n",
    "            obs, _, terminated, truncated, _ = env.step(action_vector)\n",
    "            episode_steps += 1\n",
    "\n",
    "        total_steps += episode_steps\n",
    "    return total_steps / n_episodes\n",
    "\n",
    "# ... (evaluate_survival_with_render) ...\n",
    "def evaluate_survival_with_render(model: PPO, env: gym.Env, n_episodes: int = 1) -> float:\n",
    "    total_steps = 0\n",
    "    if n_episodes <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        episode_steps = 0\n",
    "        print(f\"Starting Episode {episode + 1} with rendering...\")\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            env.render()\n",
    "\n",
    "            # For Box space, model.predict returns the continuous action vector\n",
    "            action_vector, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "            # Environment interaction\n",
    "            obs, reward, terminated, truncated, _ = env.step(action_vector)\n",
    "            episode_steps += 1\n",
    "\n",
    "            if episode_steps % 50 == 0:\n",
    "                print(f\"  Episode {episode + 1}, Step {episode_steps}, Current Score: {env.score:.1f}\")\n",
    "\n",
    "        total_steps += episode_steps\n",
    "        print(f\"Episode {episode + 1} finished after {episode_steps} steps. Final Score: {env.score:.1f}\")\n",
    "\n",
    "    env.close()\n",
    "    return total_steps / n_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280a397",
   "metadata": {},
   "source": [
    "## 3.2 Truth Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "179ffa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# 可视化评估工具\n",
    "# ========================\n",
    "class ModelVisualizer:\n",
    "    def __init__(self, perception_module: nn.Module, decision_module: nn.Module):\n",
    "        self.perception = perception_module\n",
    "        self.decision = decision_module # PPO's policy_net (mlp_extractor.policy_net)\n",
    "        \n",
    "        # --- Hooks and stored data management ---\n",
    "        # For GradCAM specific target layer\n",
    "        self.target_layer_hook_activations: Optional[torch.Tensor] = None\n",
    "        self.target_layer_hook_gradients: Optional[torch.Tensor] = None\n",
    "        \n",
    "        # For VBP/GuidedBP (and general hook management)\n",
    "        self._hook_handles: List[torch.utils.hooks.RemovableHandle] = []\n",
    "        self.module_to_forward_output: Dict[nn.Module, torch.Tensor] = {}\n",
    "\n",
    "    def cleanup_hooks(self):\n",
    "        \"\"\"Removes all registered hooks and clears stored hook-related data.\"\"\"\n",
    "        for handle in self._hook_handles:\n",
    "            handle.remove()\n",
    "        self._hook_handles = []\n",
    "        \n",
    "        self.target_layer_hook_activations = None\n",
    "        self.target_layer_hook_gradients = None\n",
    "        self.module_to_forward_output = {}\n",
    "\n",
    "    # --- Hook callback functions ---\n",
    "    def _store_forward_output_hook(self, module: nn.Module, input_val: Any, output_val: torch.Tensor):\n",
    "        \"\"\"Stores the output of a module during forward pass (for VBP).\"\"\"\n",
    "        self.module_to_forward_output[module] = output_val.detach()\n",
    "\n",
    "    def _gelu_hook_function_vbp(self, module: nn.Module, grad_input: Tuple[torch.Tensor, ...], grad_output: Tuple[torch.Tensor, ...]) -> Optional[Tuple[torch.Tensor, ...]]:\n",
    "        \"\"\"Approximated Guided Backpropagation hook for GELU.\"\"\"\n",
    "        if module not in self.module_to_forward_output:\n",
    "            # This can happen if forward pass didn't go through this specific module\n",
    "            # or if hooks were not registered correctly for the forward pass.\n",
    "            return None # Or grad_input if we want to allow passthrough\n",
    "\n",
    "        corresponding_forward_output = self.module_to_forward_output[module]\n",
    "        # Guided BP logic: only pass gradient if grad_output is positive AND forward output was positive\n",
    "        # grad_output[0] is the gradient w.r.t. the module's output.\n",
    "        # torch.clamp(grad_output[0], min=0.0) ensures only positive gradients from above are considered.\n",
    "        # (corresponding_forward_output > 0).float() ensures neuron was active.\n",
    "        guided_grad = torch.clamp(grad_output[0], min=0.0) * (corresponding_forward_output > 0).float()\n",
    "        return (guided_grad,)\n",
    "\n",
    "    def _relu_hook_function_vbp(self, module: nn.Module, grad_input: Tuple[torch.Tensor, ...], grad_output: Tuple[torch.Tensor, ...]) -> Optional[Tuple[torch.Tensor, ...]]:\n",
    "        \"\"\"Guided Backpropagation hook for ReLU.\"\"\"\n",
    "        if module not in self.module_to_forward_output:\n",
    "            return None\n",
    "            \n",
    "        corresponding_forward_output = self.module_to_forward_output[module]\n",
    "        guided_grad = torch.clamp(grad_output[0], min=0.0) * (corresponding_forward_output > 0).float()\n",
    "        return (guided_grad,)\n",
    "\n",
    "    def _gradcam_activation_hook(self, module: nn.Module, input_val: Any, output_val: torch.Tensor):\n",
    "        \"\"\"Stores activations for GradCAM.\"\"\"\n",
    "        self.target_layer_hook_activations = output_val.detach()\n",
    "\n",
    "    def _gradcam_gradient_hook(self, module: nn.Module, grad_input: Any, grad_output: Tuple[torch.Tensor, ...]):\n",
    "        \"\"\"Stores gradients for GradCAM.\"\"\"\n",
    "        self.target_layer_hook_gradients = grad_output[0].detach()\n",
    "\n",
    "    # --- Main Visualization Methods ---\n",
    "    def _register_gradcam_hooks(self):\n",
    "        \"\"\"Registers hooks for GradCAM on the target ConvNeXt layer.\"\"\"\n",
    "        self.cleanup_hooks() # Clear any previous hooks\n",
    "\n",
    "        convnext_model = None\n",
    "        if hasattr(self.perception, 'convnext'): # For PerceptionModule\n",
    "            convnext_model = self.perception.convnext\n",
    "        elif hasattr(self.perception, 'encoder'): # For PretrainedEncoderWrapper\n",
    "            convnext_model = self.perception.encoder\n",
    "        else:\n",
    "            raise TypeError(\"Perception module is of an unknown type for GradCAM hook registration.\")\n",
    "\n",
    "        try:\n",
    "            # Target the output of the last stage in ConvNeXt features\n",
    "            target_layer = convnext_model.features[-1] \n",
    "            handle_fwd = target_layer.register_forward_hook(self._gradcam_activation_hook)\n",
    "            handle_bwd = target_layer.register_full_backward_hook(self._gradcam_gradient_hook)\n",
    "            self._hook_handles.extend([handle_fwd, handle_bwd])\n",
    "        except Exception as e:\n",
    "            print(f\"Error registering GradCAM hooks: {e}. GradCAM might not work correctly.\")\n",
    "            self.cleanup_hooks() # Ensure partial hooks are removed\n",
    "    \n",
    "    def activation_maximization(self, action_idx: int, lr: float = 0.1, steps: int = 200, num_views_for_am = NUM_VIEWS) -> np.ndarray:\n",
    "        self.perception.eval()\n",
    "        self.decision.eval()\n",
    "        print(f\"Starting AM for action {action_idx}...\")\n",
    "\n",
    "        # Optimizes a single (1,3,H,W) image, assuming it's one of the N_VIEWS inputs\n",
    "        # The perception module will process it as (1, 1, C, H, W) effectively\n",
    "        # Then its features are replicated for the decision module.\n",
    "        optimized_image_tensor = torch.rand(1, 3, 100, 100, requires_grad=True, device=next(self.perception.parameters()).device)\n",
    "        optimizer = optim.Adam([optimized_image_tensor], lr=lr, weight_decay=1e-4)\n",
    "\n",
    "        for i in range(steps):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Clamp and ensure image is in [0,1] range for perception module\n",
    "            current_image_0_1 = torch.clamp(optimized_image_tensor, 0.0, 1.0)\n",
    "            \n",
    "            # Perception module expects (B, N_VIEWS, C, H, W)\n",
    "            # We form an input where one view is the optimized image, others could be neutral (e.g., gray)\n",
    "            # For simplicity in AM: assume the optimized image is so dominant it works if it's just one view.\n",
    "            # The self.perception here is the SB3 model's feature extractor's perception part.\n",
    "            # It expects (B, N_VIEWS, C, H, W) format.\n",
    "            # So, let's treat the optimized image as if it's all N_VIEWS for AM purposes.\n",
    "            multi_view_input = current_image_0_1.repeat(1, num_views_for_am, 1, 1, 1).squeeze(0) # (N_VIEWS, C, H, W)\n",
    "            multi_view_input = multi_view_input.unsqueeze(0) # (1, N_VIEWS, C, H, W)\n",
    "\n",
    "\n",
    "            # Get features from perception ( (1, N_VIEWS, feat_dim) )\n",
    "            features_per_view = self.perception(multi_view_input)\n",
    "            # Flatten for decision module ( (1, N_VIEWS * feat_dim) )\n",
    "            flat_features = features_per_view.view(1, -1)\n",
    "            \n",
    "            action_distribution = self.decision(flat_features) # decision is PPO's policy_net\n",
    "            \n",
    "            loss = -action_distribution[0, action_idx] # Maximize prob of this action\n",
    "            \n",
    "            # Add some regularization to the image (e.g., total variation)\n",
    "            loss += 0.0001 * torch.sum(torch.abs(current_image_0_1[:, :, :, :-1] - current_image_0_1[:, :, :, 1:])) + \\\n",
    "                    0.0001 * torch.sum(torch.abs(current_image_0_1[:, :, :-1, :] - current_image_0_1[:, :, 1:, :]))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % (steps // 10) == 0:\n",
    "                 print(f\"AM step {i}, loss {loss.item()}\")\n",
    "\n",
    "        final_image_0_1 = torch.clamp(optimized_image_tensor.detach(), 0.0, 1.0)\n",
    "        generated_np = final_image_0_1.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "        return (generated_np * 255).astype(np.uint8)\n",
    "\n",
    "    def grad_cam(self, obs_tensor_0_1: torch.Tensor, action_idx: int, target_view_idx: int = 0) -> Optional[np.ndarray]:\n",
    "        # obs_tensor_0_1 is (1, N_VIEWS, C, H, W), scaled [0,1]\n",
    "        # target_view_idx specifies which of the N_VIEWS to generate GradCAM for.\n",
    "        self.perception.eval()\n",
    "        self.decision.eval()\n",
    "        self._register_gradcam_hooks() # Ensure hooks are on the correct layer\n",
    "        \n",
    "        if not self._hook_handles: # Check if hooks were successfully registered\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        obs_tensor_0_1.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass\n",
    "        # self.perception is the perception module from the SB3 agent (e.g., PerceptionModule or PretrainedEncoderWrapper)\n",
    "        features_per_view = self.perception(obs_tensor_0_1) # (1, N_VIEWS, feat_dim)\n",
    "        flat_features = features_per_view.view(1, -1)       # (1, N_VIEWS * feat_dim)\n",
    "        \n",
    "        # self.decision is the PPO's policy_net\n",
    "        action_distribution = self.decision(flat_features) # (1, num_actions)\n",
    "        \n",
    "        # Backward pass for the target action\n",
    "        self.perception.zero_grad() # Zero grads for perception module's ConvNeXt\n",
    "        if self.decision.parameters(): # Also zero grads for decision MLP if it has params\n",
    "            self.decision.zero_grad()\n",
    "\n",
    "        score = action_distribution[0, action_idx]\n",
    "        score.backward(retain_graph=True)\n",
    "\n",
    "        if self.target_layer_hook_activations is None or self.target_layer_hook_gradients is None:\n",
    "            print(\"GradCAM: Activations or gradients not captured. Hooks might not be set correctly.\")\n",
    "            return None\n",
    "\n",
    "        # Activations/Gradients are from the ConvNeXt internal layer, shape (N_VIEWS_eff, C_feat, H_feat, W_feat)\n",
    "        # N_VIEWS_eff is batch_size * num_views from the perception module's internal reshaping. Here batch_size=1.\n",
    "        activations_all_views = self.target_layer_hook_activations # (NUM_VIEWS, C_feat, H_feat, W_feat)\n",
    "        gradients_all_views = self.target_layer_hook_gradients     # (NUM_VIEWS, C_feat, H_feat, W_feat)\n",
    "\n",
    "        # Select the specific view\n",
    "        activations_target_view = activations_all_views[target_view_idx] # (C_feat, H_feat, W_feat)\n",
    "        gradients_target_view = gradients_all_views[target_view_idx]   # (C_feat, H_feat, W_feat)\n",
    "        \n",
    "        # Compute weights (alpha_k)\n",
    "        pooled_gradients = torch.mean(gradients_target_view, dim=[1, 2]) # (C_feat)\n",
    "        \n",
    "        # Weight activations\n",
    "        for i in range(activations_target_view.shape[0]): # Loop over channels\n",
    "            activations_target_view[i, :, :] *= pooled_gradients[i]\n",
    "            \n",
    "        heatmap = torch.mean(activations_target_view, dim=0).cpu().numpy() # (H_feat, W_feat)\n",
    "        heatmap = np.maximum(heatmap, 0) # ReLU\n",
    "        if np.max(heatmap) > 0:\n",
    "            heatmap /= np.max(heatmap) # Normalize\n",
    "        \n",
    "        # Resize to original image size\n",
    "        original_h, original_w = obs_tensor_0_1.shape[-2:]\n",
    "        heatmap_resized = cv2.resize(heatmap, (original_w, original_h))\n",
    "        return heatmap_resized\n",
    "    \n",
    "    def visual_back_prop(self, obs_tensor_0_1: torch.Tensor, target_view_idx: int = 0) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generates a VisualBackProp (Guided Backpropagation style) saliency map.\n",
    "        Shows general input patterns contributing to the perception module's features for a view.\n",
    "        \"\"\"\n",
    "        self.perception.eval()\n",
    "        self.cleanup_hooks() # Clears all hooks and stored data (module_to_forward_output too)\n",
    "\n",
    "        convnext_model = None\n",
    "        if hasattr(self.perception, 'convnext'):\n",
    "            convnext_model = self.perception.convnext\n",
    "        elif hasattr(self.perception, 'encoder'):\n",
    "            convnext_model = self.perception.encoder\n",
    "        else:\n",
    "            print(\"VBP: Perception module type not recognized.\")\n",
    "            return None\n",
    "\n",
    "        # Register VBP hooks on all GELU/ReLU layers in the ConvNeXt model\n",
    "        for module_name, module in convnext_model.named_modules():\n",
    "            if isinstance(module, nn.GELU):\n",
    "                self._hook_handles.append(module.register_forward_hook(self._store_forward_output_hook))\n",
    "                self._hook_handles.append(module.register_full_backward_hook(self._gelu_hook_function_vbp))\n",
    "            elif isinstance(module, nn.ReLU): # Fallback for any ReLUs\n",
    "                self._hook_handles.append(module.register_forward_hook(self._store_forward_output_hook))\n",
    "                self._hook_handles.append(module.register_full_backward_hook(self._relu_hook_function_vbp))\n",
    "        \n",
    "        if not self._hook_handles:\n",
    "            print(\"VBP: No suitable activation layers (GELU/ReLU) found to hook in ConvNeXt.\")\n",
    "            return None # cleanup_hooks already called, so state is clean.\n",
    "\n",
    "        # Prepare input image: clone, detach, and set requires_grad\n",
    "        input_img_for_vbp = obs_tensor_0_1.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        # --- Forward pass ---\n",
    "        # This single forward pass will:\n",
    "        # 1. Populate self.module_to_forward_output via the forward hooks.\n",
    "        # 2. Give us features_per_view to backpropagate from.\n",
    "        features_per_view = self.perception(input_img_for_vbp) # Output: (B, N_VIEWS, feature_dim)\n",
    "        \n",
    "        # --- Backward pass ---\n",
    "        self.perception.zero_grad() # Zero gradients for the perception model parameters\n",
    "        if input_img_for_vbp.grad is not None:\n",
    "            input_img_for_vbp.grad.data.zero_()\n",
    "\n",
    "        # Target for backpropagation: Sum of features for the target_view_idx.\n",
    "        # This gives a \"general pattern\" for that view's features.\n",
    "        target_features_for_bp = features_per_view[0, target_view_idx, :] # Shape: (feature_dim)\n",
    "        \n",
    "        # Gradient for backward pass (sum of features -> gradient of 1 for each feature)\n",
    "        grad_outputs_for_bp = torch.ones_like(target_features_for_bp)\n",
    "        \n",
    "        target_features_for_bp.backward(gradient=grad_outputs_for_bp, retain_graph=False)\n",
    "\n",
    "        # --- Retrieve and process gradient on the input image ---\n",
    "        saliency_grad = input_img_for_vbp.grad\n",
    "        if saliency_grad is None:\n",
    "            print(\"VBP: Gradient for input image was not computed.\")\n",
    "            # Hooks will be cleaned up by the next call to a viz method or external `visualizer.cleanup_hooks()`\n",
    "            return None\n",
    "\n",
    "        # Take absolute value of gradients for the target view\n",
    "        saliency_abs = saliency_grad.data.abs() # Shape: (1, N_VIEWS, C, H, W)\n",
    "        saliency_target_view_abs = saliency_abs[0, target_view_idx, :, :, :] # Shape: (C, H, W)\n",
    "        \n",
    "        saliency_target_view_np = saliency_target_view_abs.cpu().numpy()\n",
    "        \n",
    "        # Normalize: max across channels, then scale to [0, 255]\n",
    "        saliency_map = np.max(saliency_target_view_np, axis=0) # Shape: (H, W)\n",
    "        if np.max(saliency_map) > 0:\n",
    "            saliency_map /= np.max(saliency_map)\n",
    "        saliency_map_uint8 = (saliency_map * 255).astype(np.uint8)\n",
    "        \n",
    "        # Hooks will be cleaned up by the next call to a viz method or an explicit call to self.cleanup_hooks().\n",
    "        return saliency_map_uint8\n",
    "    def cleanup_hooks(self):\n",
    "        for handle in self._hook_handles:\n",
    "            handle.remove()\n",
    "        self._hook_handles = []\n",
    "\n",
    "\n",
    "# ========================\n",
    "# 可视化评估函数\n",
    "# ========================\n",
    "def visualize_truth_evaluation(\n",
    "    model: PPO,\n",
    "    # MODIFIED: Now takes a specific subset of images to evaluate\n",
    "    evaluation_image_subset: Dict[str, List[Tuple[np.ndarray, int]]], # List of (image_array, original_index_or_id)\n",
    "    generic_env_img: np.ndarray, # Pass the pre-selected generic environment image\n",
    "    env: gym.Env, # Still useful for some observation space properties if needed\n",
    "    run_name=\"default\"\n",
    "):\n",
    "    print(f\"\\nPerforming Truth visualization evaluation for {run_name}...\")\n",
    "\n",
    "    device = model.device # Get device from the model\n",
    "\n",
    "    feature_extractor = model.policy.features_extractor\n",
    "    if not hasattr(feature_extractor, 'perception'):\n",
    "        print(\"Error: PPO model's feature_extractor does not have a 'perception' attribute.\")\n",
    "        return\n",
    "\n",
    "    perception_module_from_agent = feature_extractor.perception\n",
    "    decision_module_from_agent = model.policy.mlp_extractor.policy_net\n",
    "\n",
    "    visualizer = ModelVisualizer(perception_module_from_agent, decision_module_from_agent)\n",
    "    action_names = ['Escape', 'Eat', 'Wander']\n",
    "\n",
    "    # 1. Activation Maximization (run once per model, as it generates images)\n",
    "    print(f\"[{run_name}] Generating Activation Maximization visualizations...\")\n",
    "    am_output_dir = f\"am_results_{run_name}\"\n",
    "    os.makedirs(am_output_dir, exist_ok=True)\n",
    "    for action_idx, action_name in enumerate(action_names):\n",
    "        print(f\"  Running AM for intent: {action_name}\")\n",
    "        am_image_np = visualizer.activation_maximization(action_idx, steps=200)\n",
    "        if am_image_np is not None:\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.imshow(am_image_np)\n",
    "            plt.title(f'AM ({run_name}): Target Intent Neuron {action_name}')\n",
    "            plt.axis('off')\n",
    "            plt.savefig(os.path.join(am_output_dir, f'am_{action_name.lower()}.png'))\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(f\"  AM failed for intent: {action_name}\")\n",
    "\n",
    "    # --- Specific Image Evaluation ---\n",
    "    print(f\"[{run_name}] Generating Saliency Maps for specific image types...\")\n",
    "\n",
    "    saliency_output_dir = f\"saliency_results_{run_name}\"\n",
    "    os.makedirs(saliency_output_dir, exist_ok=True)\n",
    "\n",
    "    # MODIFIED: Iterate through the pre-selected subset of images\n",
    "    for img_type_name, images_to_eval in evaluation_image_subset.items():\n",
    "        print(f\"  Evaluating image type: {img_type_name} ({len(images_to_eval)} images)\")\n",
    "        if not images_to_eval:\n",
    "            print(f\"    Skipping '{img_type_name}', no images provided in subset.\")\n",
    "            continue\n",
    "\n",
    "        for target_image_np, img_idx_or_id in images_to_eval: # img_idx_or_id for unique naming\n",
    "            print(f\"    Processing image index/id: {img_idx_or_id} for type {img_type_name}\")\n",
    "            # Construct the 4-view observation: target image in front, others generic\n",
    "            current_obs_np = np.stack([\n",
    "                target_image_np,    # Front view (our target image)\n",
    "                generic_env_img,    # Left view\n",
    "                generic_env_img,    # Right view\n",
    "                generic_env_img     # Back view\n",
    "            ])\n",
    "\n",
    "            obs_tensor_0_1 = torch.tensor(current_obs_np, dtype=torch.float32, device=device)\n",
    "            obs_tensor_0_1 = obs_tensor_0_1.permute(0, 3, 1, 2).unsqueeze(0) / 255.0\n",
    "\n",
    "            target_view_idx_for_viz = 0\n",
    "\n",
    "            # 2. Saliency Maps (GradCAM) - For each of the 3 intents\n",
    "            # print(f\"    Generating GradCAM for {img_type_name} image {img_idx_or_id}...\") # Verbose\n",
    "            for intent_idx, intent_name in enumerate(action_names):\n",
    "                grad_cam_map = visualizer.grad_cam(obs_tensor_0_1.clone(), intent_idx, target_view_idx=target_view_idx_for_viz)\n",
    "\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                # MODIFIED: More specific title and filename\n",
    "                title = f'GradCAM ({run_name}) - Img: {img_type_name}_{img_idx_or_id}, Intent: {intent_name}'\n",
    "                filename = f'gradcam_{img_type_name}_{img_idx_or_id}_intent_{intent_name.lower()}.png'\n",
    "                plt.suptitle(title, fontsize=14)\n",
    "\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.imshow(target_image_np)\n",
    "                plt.title(f'Input: {img_type_name} (ID: {img_idx_or_id})')\n",
    "                plt.axis('off')\n",
    "\n",
    "                if grad_cam_map is not None:\n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.imshow(target_image_np, alpha=0.7)\n",
    "                    plt.imshow(grad_cam_map, cmap='jet', alpha=0.3)\n",
    "                    plt.title(f'GradCAM Overlay')\n",
    "                    plt.axis('off')\n",
    "                else:\n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.text(0.5, 0.5, \"GradCAM Failed\", ha='center', va='center')\n",
    "                    plt.axis('off')\n",
    "\n",
    "                plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "                plt.savefig(os.path.join(saliency_output_dir, filename))\n",
    "                plt.close()\n",
    "\n",
    "            # 3. VisualBackProp\n",
    "            # print(f\"    Generating VisualBackProp for {img_type_name} image {img_idx_or_id}...\") # Verbose\n",
    "            vbp_map = visualizer.visual_back_prop(obs_tensor_0_1.clone(), target_view_idx=target_view_idx_for_viz)\n",
    "\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            # MODIFIED: More specific title and filename\n",
    "            title = f'VisualBackProp ({run_name}) - Img: {img_type_name}_{img_idx_or_id}'\n",
    "            filename = f'vbp_{img_type_name}_{img_idx_or_id}.png'\n",
    "            plt.suptitle(title, fontsize=14)\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(target_image_np)\n",
    "            plt.title(f'Input: {img_type_name} (ID: {img_idx_or_id})')\n",
    "            plt.axis('off')\n",
    "\n",
    "            if vbp_map is not None:\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.imshow(vbp_map, cmap='gray')\n",
    "                plt.title('VBP Saliency')\n",
    "                plt.axis('off')\n",
    "            else:\n",
    "                plt.subplot(1,2,2)\n",
    "                plt.text(0.5, 0.5, \"VBP Failed\", ha='center', va='center')\n",
    "                plt.axis('off')\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "            plt.savefig(os.path.join(saliency_output_dir, filename))\n",
    "            plt.close()\n",
    "\n",
    "    visualizer.cleanup_hooks() # Final cleanup\n",
    "    print(f\"Truth visualization for {run_name} (specific images) complete. AM results in '{am_output_dir}', Saliency in '{saliency_output_dir}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395fe687",
   "metadata": {},
   "source": [
    "# 4. 主工作流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22afff1",
   "metadata": {},
   "source": [
    "## 4.1 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa93c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 folders\n",
      "environment: 22736 images, shape=(100, 100, 3)\n",
      "food: 102790 images, shape=(100, 100, 3)\n",
      "predator: 861 images, shape=(100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========================\n",
    "# 主工作流程\n",
    "# ========================\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_TIMESTEPS = 20480 # Very short for testing, increase for real training (e.g., 50000+)\n",
    "\n",
    "base_directory = \"TrainingData\"  # 替换为你的文件夹路径\n",
    "image_datasets = load_images_to_dict(base_directory)\n",
    "# 打印结构信息\n",
    "print(f\"Loaded {len(image_datasets)} folders\")\n",
    "for folder, images in image_datasets.items():\n",
    "    print(f\"{folder}: {len(images)} images, shape={images[0].shape if images else 'N/A'}\")\n",
    "\n",
    "env = SurvivalGameEnv(image_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e99f1",
   "metadata": {},
   "source": [
    "## 4.2 预训练自编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ad41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pretrain Autoencoder\n",
    "# print(\"Pretraining Autoencoder...\")\n",
    "# autoencoder_model = train_autoencoder(image_datasets, epochs=5, ae_save_path=\"autoencoder.pth\") # Short epochs for test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee1bdf6",
   "metadata": {},
   "source": [
    "## 4.3 训练Fitness模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Fitness model\n",
    "print(\"\\nTraining Fitness model...\")\n",
    "fitness_model = fitness_training(env, total_timesteps=TRAIN_TIMESTEPS)\n",
    "fitness_model.save(\"ppo_fitness_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac82e189",
   "metadata": {},
   "source": [
    "## 4.4 训练Truth模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Truth model\n",
    "print(\"\\nTraining Truth model...\")\n",
    "truth_model = truth_training(env, ae_path=\"autoencoder.pth\", total_timesteps=TRAIN_TIMESTEPS)\n",
    "truth_model.save(\"ppo_truth_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db70784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45984e9b",
   "metadata": {},
   "source": [
    "## 4.5 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9baf773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "Evaluating models for survival...\n",
      "\n",
      "Results:\n",
      "Fitness Model Average Survival: 143.4 steps\n",
      "Truth Model Average Survival: 199.2 steps\n",
      "\\nPreparing images for Truth visualization evaluation...\n",
      "\n",
      "Performing Truth visualization evaluation for FitnessModel...\n",
      "[FitnessModel] Generating Activation Maximization visualizations...\n",
      "  Running AM for intent: Escape\n",
      "Starting AM for action 0...\n",
      "AM step 0, loss 2.053948402404785\n",
      "AM step 20, loss 0.3078014850616455\n",
      "AM step 40, loss 0.18436838686466217\n",
      "AM step 60, loss 0.08877288550138474\n",
      "AM step 80, loss 0.08494089543819427\n",
      "AM step 100, loss 0.08495432138442993\n",
      "AM step 120, loss 0.0851912647485733\n",
      "AM step 140, loss 0.08495713025331497\n",
      "AM step 160, loss 0.08518572151660919\n",
      "AM step 180, loss 0.08520476520061493\n",
      "  Running AM for intent: Eat\n",
      "Starting AM for action 1...\n",
      "AM step 0, loss 2.029921770095825\n",
      "AM step 20, loss 0.2604913115501404\n",
      "AM step 40, loss 0.13519862294197083\n",
      "AM step 60, loss 0.03846631944179535\n",
      "AM step 80, loss 0.03429092466831207\n",
      "AM step 100, loss 0.03430471569299698\n",
      "AM step 120, loss 0.0345458984375\n",
      "AM step 140, loss 0.03430778905749321\n",
      "AM step 160, loss 0.0345223993062973\n",
      "AM step 180, loss 0.03452995792031288\n",
      "  Running AM for intent: Wander\n",
      "Starting AM for action 2...\n",
      "AM step 0, loss 2.8384485244750977\n",
      "AM step 20, loss 1.084893822669983\n",
      "AM step 40, loss 0.9600987434387207\n",
      "AM step 60, loss 0.8626084923744202\n",
      "AM step 80, loss 0.8595148921012878\n",
      "AM step 100, loss 0.8595332503318787\n",
      "AM step 120, loss 0.8597506284713745\n",
      "AM step 140, loss 0.8595346212387085\n",
      "AM step 160, loss 0.8597465753555298\n",
      "AM step 180, loss 0.859755277633667\n",
      "[FitnessModel] Generating Saliency Maps for specific image types...\n",
      "  Evaluating image type: predator (10 images)\n",
      "    Processing image index/id: 703 for type predator\n",
      "    Processing image index/id: 113 for type predator\n",
      "    Processing image index/id: 812 for type predator\n",
      "    Processing image index/id: 445 for type predator\n",
      "    Processing image index/id: 538 for type predator\n",
      "    Processing image index/id: 741 for type predator\n",
      "    Processing image index/id: 548 for type predator\n",
      "    Processing image index/id: 776 for type predator\n",
      "    Processing image index/id: 369 for type predator\n",
      "    Processing image index/id: 11 for type predator\n",
      "  Evaluating image type: food (10 images)\n",
      "    Processing image index/id: 87753 for type food\n",
      "    Processing image index/id: 35184 for type food\n",
      "    Processing image index/id: 47729 for type food\n",
      "    Processing image index/id: 7170 for type food\n",
      "    Processing image index/id: 79850 for type food\n",
      "    Processing image index/id: 102069 for type food\n",
      "    Processing image index/id: 97756 for type food\n",
      "    Processing image index/id: 36353 for type food\n",
      "    Processing image index/id: 78822 for type food\n",
      "    Processing image index/id: 70642 for type food\n",
      "  Evaluating image type: environment (10 images)\n",
      "    Processing image index/id: 10150 for type environment\n",
      "    Processing image index/id: 693 for type environment\n",
      "    Processing image index/id: 15599 for type environment\n",
      "    Processing image index/id: 18562 for type environment\n",
      "    Processing image index/id: 20609 for type environment\n",
      "    Processing image index/id: 6842 for type environment\n",
      "    Processing image index/id: 19210 for type environment\n",
      "    Processing image index/id: 18219 for type environment\n",
      "    Processing image index/id: 8115 for type environment\n",
      "    Processing image index/id: 6127 for type environment\n",
      "Truth visualization for FitnessModel (specific images) complete. AM results in 'am_results_FitnessModel', Saliency in 'saliency_results_FitnessModel'.\n",
      "\n",
      "Performing Truth visualization evaluation for TruthModel...\n",
      "[TruthModel] Generating Activation Maximization visualizations...\n",
      "  Running AM for intent: Escape\n",
      "Starting AM for action 0...\n",
      "AM step 0, loss 1.207504153251648\n",
      "AM step 20, loss -0.7053043842315674\n",
      "AM step 40, loss -0.946506142616272\n",
      "AM step 60, loss -0.9753456115722656\n",
      "AM step 80, loss -0.9425796866416931\n",
      "AM step 100, loss -0.9232529401779175\n",
      "AM step 120, loss -0.9479100108146667\n",
      "AM step 140, loss -0.934940755367279\n",
      "AM step 160, loss -0.9419882297515869\n",
      "AM step 180, loss -0.9264792799949646\n",
      "  Running AM for intent: Eat\n",
      "Starting AM for action 1...\n",
      "AM step 0, loss 1.9260457754135132\n",
      "AM step 20, loss -0.5673305988311768\n",
      "AM step 40, loss -0.9737831354141235\n",
      "AM step 60, loss -0.9494628310203552\n",
      "AM step 80, loss -0.9210055470466614\n",
      "AM step 100, loss -0.9351039528846741\n",
      "AM step 120, loss -0.9351581931114197\n",
      "AM step 140, loss -0.9309609532356262\n",
      "AM step 160, loss -0.9287838935852051\n",
      "AM step 180, loss -0.9237560033798218\n",
      "  Running AM for intent: Wander\n",
      "Starting AM for action 2...\n",
      "AM step 0, loss 2.049649477005005\n",
      "AM step 20, loss -0.5721629858016968\n",
      "AM step 40, loss -0.9740204215049744\n",
      "AM step 60, loss -0.9419187307357788\n",
      "AM step 80, loss -0.9195409417152405\n",
      "AM step 100, loss -0.9336759448051453\n",
      "AM step 120, loss -0.9282958507537842\n",
      "AM step 140, loss -0.9218154549598694\n",
      "AM step 160, loss -0.9213463068008423\n",
      "AM step 180, loss -0.9122015833854675\n",
      "[TruthModel] Generating Saliency Maps for specific image types...\n",
      "  Evaluating image type: predator (10 images)\n",
      "    Processing image index/id: 703 for type predator\n",
      "    Processing image index/id: 113 for type predator\n",
      "    Processing image index/id: 812 for type predator\n",
      "    Processing image index/id: 445 for type predator\n",
      "    Processing image index/id: 538 for type predator\n",
      "    Processing image index/id: 741 for type predator\n",
      "    Processing image index/id: 548 for type predator\n",
      "    Processing image index/id: 776 for type predator\n",
      "    Processing image index/id: 369 for type predator\n",
      "    Processing image index/id: 11 for type predator\n",
      "  Evaluating image type: food (10 images)\n",
      "    Processing image index/id: 87753 for type food\n",
      "    Processing image index/id: 35184 for type food\n",
      "    Processing image index/id: 47729 for type food\n",
      "    Processing image index/id: 7170 for type food\n",
      "    Processing image index/id: 79850 for type food\n",
      "    Processing image index/id: 102069 for type food\n",
      "    Processing image index/id: 97756 for type food\n",
      "    Processing image index/id: 36353 for type food\n",
      "    Processing image index/id: 78822 for type food\n",
      "    Processing image index/id: 70642 for type food\n",
      "  Evaluating image type: environment (10 images)\n",
      "    Processing image index/id: 10150 for type environment\n",
      "    Processing image index/id: 693 for type environment\n",
      "    Processing image index/id: 15599 for type environment\n",
      "    Processing image index/id: 18562 for type environment\n",
      "    Processing image index/id: 20609 for type environment\n",
      "    Processing image index/id: 6842 for type environment\n",
      "    Processing image index/id: 19210 for type environment\n",
      "    Processing image index/id: 18219 for type environment\n",
      "    Processing image index/id: 8115 for type environment\n",
      "    Processing image index/id: 6127 for type environment\n",
      "Truth visualization for TruthModel (specific images) complete. AM results in 'am_results_TruthModel', Saliency in 'saliency_results_TruthModel'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load models if needed (example)\n",
    "fitness_model = PPO.load(\"ppo_fitness_model\", env=env)\n",
    "truth_model = PPO.load(\"ppo_truth_model\", env=env)\n",
    "\n",
    "# Evaluate models\n",
    "print(\"\\nEvaluating models for survival...\")\n",
    "# 非实时渲染fitness模型评估\n",
    "fitness_score = evaluate_survival(fitness_model, env, n_episodes=5)\n",
    "truth_score = evaluate_survival(truth_model, env, n_episodes=5)\n",
    "# 实时渲染fitness模型评估\n",
    "# fitness_score = evaluate_survival_with_render(fitness_model, env, n_episodes=1)\n",
    "# truth_score = evaluate_survival_with_render(truth_model, env, n_episodes=1)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Fitness Model Average Survival: {fitness_score:.1f} steps\")\n",
    "print(f\"Truth Model Average Survival: {truth_score:.1f} steps\")\n",
    "\n",
    "if 'image_datasets' not in locals():\n",
    "    print(\"Reloading image_datasets for visualization...\")\n",
    "    base_directory = \"TrainingData\" \n",
    "    image_datasets = load_images_to_dict(base_directory)\n",
    "# Truth visualization evaluation\n",
    "print(\"\\\\nPreparing images for Truth visualization evaluation...\")\n",
    "N_IMAGES_PER_CATEGORY = 10\n",
    "evaluation_image_subset = {}\n",
    "image_categories_for_viz = ['predator', 'food', 'environment']\n",
    "\n",
    "for category in image_categories_for_viz:\n",
    "    if category in image_datasets and image_datasets[category]:\n",
    "        available_images = image_datasets[category]\n",
    "        num_to_sample = min(N_IMAGES_PER_CATEGORY, len(available_images))\n",
    "        if len(available_images) < N_IMAGES_PER_CATEGORY:\n",
    "            print(f\"Warning: Category '{category}' has only {len(available_images)} images. Sampling all of them.\")\n",
    "        \n",
    "        # Sample 'num_to_sample' images. random.sample ensures no replacement.\n",
    "        # Store as (image_array, original_index_within_category_list_for_uniqueness)\n",
    "        # If you need globally unique IDs, you might need a different strategy for 'idx'\n",
    "        sampled_indices = random.sample(range(len(available_images)), num_to_sample)\n",
    "        evaluation_image_subset[category] = [(available_images[i], i) for i in sampled_indices]\n",
    "    else:\n",
    "        print(f\"Warning: No images found for category '{category}'. It will be skipped in visualization.\")\n",
    "        evaluation_image_subset[category] = []\n",
    "\n",
    "# Select a generic environment image for filler views\n",
    "generic_env_img_list = image_datasets.get('environment')\n",
    "if not generic_env_img_list:\n",
    "    print(\"Critical Warning: No 'environment' images found for generic filler. Using a black image.\")\n",
    "    # Assuming image shape from observation_space or a default\n",
    "    h_obs, w_obs, c_obs = (100,100,3) \n",
    "    if hasattr(env.observation_space, 'shape') and len(env.observation_space.shape) == 4:\n",
    "         h_obs, w_obs, c_obs = env.observation_space.shape[1:4]\n",
    "    generic_env_img = np.zeros((h_obs, w_obs, c_obs), dtype=np.uint8)\n",
    "else:\n",
    "    generic_env_img = random.choice(generic_env_img_list)\n",
    "\n",
    "\n",
    "# Truth visualization evaluation for Fitness Model\n",
    "visualize_truth_evaluation(\n",
    "    fitness_model, \n",
    "    evaluation_image_subset, \n",
    "    generic_env_img,\n",
    "    env, \n",
    "    run_name=\"FitnessModel\"\n",
    ")\n",
    "\n",
    "# Truth visualization evaluation for Truth Model\n",
    "visualize_truth_evaluation(\n",
    "    truth_model, \n",
    "    evaluation_image_subset, \n",
    "    generic_env_img,\n",
    "    env, \n",
    "    run_name=\"TruthModel\"\n",
    ")\n",
    "\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
