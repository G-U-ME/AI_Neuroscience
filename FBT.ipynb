{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8cc4a28",
   "metadata": {},
   "source": [
    "# 0. 导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37032d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import BaseFeaturesExtractor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Optional, Any\n",
    "import cv2\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "def load_images_to_dict(base_dir: str) -> Dict[str, List[np.ndarray]]:\n",
    "    image_datasets = {}\n",
    "    # 定义支持的图片扩展名\n",
    "    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif'}\n",
    "    \n",
    "    # 遍历基础目录下的所有子文件夹\n",
    "    for folder_name in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "        \n",
    "        # 确保是文件夹\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        \n",
    "        image_list = []\n",
    "        \n",
    "        # 遍历文件夹中的文件\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # 检查文件扩展名\n",
    "            ext = os.path.splitext(filename)[1].lower()\n",
    "            if ext not in valid_extensions:\n",
    "                continue  # 跳过非图片文件\n",
    "            \n",
    "            try:\n",
    "                # 用PIL打开图片并转换为RGB（避免Alpha通道问题）\n",
    "                with Image.open(file_path) as img:\n",
    "                    img = img.convert('RGB')  # 统一转换为RGB三通道\n",
    "                    image_list.append(np.array(img))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # 将文件夹名作为key，图片数组列表作为value\n",
    "        image_datasets[folder_name] = image_list\n",
    "    \n",
    "    return image_datasets\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_list: List[np.ndarray], transform: Any = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_list (List[np.ndarray]): List of images as NumPy arrays.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.images = image_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        image_np = self.images[idx]\n",
    "\n",
    "        # Ensure image is in uint8 format for PIL conversion if necessary,\n",
    "        # or if ToTensor is directly applied to numpy array.\n",
    "        if image_np.dtype != np.uint8:\n",
    "            # This can happen if images were, e.g., float32 from some processing.\n",
    "            # Assuming they were originally 0-255 range if they are not uint8.\n",
    "            # If they are already 0-1 float, this needs adjustment.\n",
    "            # For typical image data, astype(np.uint8) is safe.\n",
    "            image_np = image_np.astype(np.uint8)\n",
    "\n",
    "        # transforms.ToTensor() can handle HWC uint8 NumPy arrays directly.\n",
    "        # It will convert to CHW FloatTensor and scale to [0.0, 1.0].\n",
    "        # If you prefer to use PIL Image explicitly:\n",
    "        # image_pil = Image.fromarray(image_np)\n",
    "        # if self.transform:\n",
    "        #     image_tensor = self.transform(image_pil)\n",
    "        # else: # Fallback if no transform, though ToTensor is crucial\n",
    "        #     image_tensor = transforms.ToTensor()(image_pil)\n",
    "\n",
    "        # Direct application to NumPy array (preferred if array is HWC)\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_np)\n",
    "        else:\n",
    "            # Default to ToTensor if no specific transform is given,\n",
    "            # as it's essential for PyTorch models.\n",
    "            image_tensor = transforms.ToTensor()(image_np)\n",
    "\n",
    "        return image_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f63e2",
   "metadata": {},
   "source": [
    "# 1. Survival Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a738045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# 环境配置\n",
    "# ========================\n",
    "MAP_SIZE = 20\n",
    "PREDATOR_COUNT = 5\n",
    "MAX_FOOD = 50\n",
    "CLUSTER_RADIUS = 3\n",
    "PIXEL_TYPES = {\n",
    "    0: 'environment',\n",
    "    1: 'predator',\n",
    "    2: 'food',\n",
    "    3: 'agent'\n",
    "}\n",
    "REWARDS = {\n",
    "    'predator': -50,  # 调整为文档要求的-50\n",
    "    'food': +10,      # 调整为文档要求的+10\n",
    "    'environment': -1\n",
    "}\n",
    "NUM_VIEWS = 4  # Front, Left, Right, Back\n",
    "INITIAL_SCORE = 100  # 添加初始分数\n",
    "\n",
    "# ========================\n",
    "# 环境实现\n",
    "# ========================\n",
    "class SurvivalGameEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': 4}\n",
    "\n",
    "    def __init__(self, image_datasets: Dict[str, List[np.ndarray]]):\n",
    "        super(SurvivalGameEnv, self).__init__()\n",
    "\n",
    "        self.image_datasets = image_datasets\n",
    "        # This transform is for external use if needed, model handles its own\n",
    "        self.vis_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)  # Escape, Eat, Wander\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255,\n",
    "            shape=(NUM_VIEWS, 100, 100, 3),  # (4 views, H, W, C)\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        self.current_map_image = None  # For rendering\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "        super().reset(seed=seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # 初始化地图\n",
    "        self.map = np.zeros((MAP_SIZE, MAP_SIZE), dtype=np.uint8)\n",
    "        self.predators = []\n",
    "        self.foods = {}\n",
    "        \n",
    "        # 放置捕食者\n",
    "        for _ in range(PREDATOR_COUNT):\n",
    "            pos = self._random_edge_position()\n",
    "            while self.map[pos] != 0:  # 确保位置为空\n",
    "                pos = self._random_edge_position()\n",
    "            self.map[pos] = 1\n",
    "            self.predators.append({\n",
    "                'pos': pos,\n",
    "                'img_idx': np.random.randint(len(self.image_datasets['predator']))\n",
    "            })\n",
    "        \n",
    "        # 集群式生成食物\n",
    "        num_clusters = max(1, MAX_FOOD // 10)  # 每10个食物一个集群\n",
    "        cluster_centers = [self._random_position() for _ in range(num_clusters)]\n",
    "        \n",
    "        for center in cluster_centers:\n",
    "            # 使用高斯分布生成集群\n",
    "            cluster_size = min(MAX_FOOD - len(self.foods), MAX_FOOD // num_clusters)\n",
    "            for _ in range(cluster_size):\n",
    "                # 在集群半径内生成随机偏移\n",
    "                dx = int(np.random.normal(0, CLUSTER_RADIUS / 2))\n",
    "                dy = int(np.random.normal(0, CLUSTER_RADIUS / 2))\n",
    "                pos = (\n",
    "                    np.clip(center[0] + dx, 0, MAP_SIZE - 1),\n",
    "                    np.clip(center[1] + dy, 0, MAP_SIZE - 1)\n",
    "                )\n",
    "                \n",
    "                if self._is_valid_position(pos) and self.map[pos] == 0:\n",
    "                    self.map[pos] = 2\n",
    "                    self.foods[pos] = {\n",
    "                        'img_idx': np.random.randint(len(self.image_datasets['food'])),\n",
    "                        'consumed': False\n",
    "                    }\n",
    "                    if len(self.foods) >= MAX_FOOD:\n",
    "                        break\n",
    "            if len(self.foods) >= MAX_FOOD:\n",
    "                break\n",
    "\n",
    "        # 放置代理\n",
    "        self.agent_pos = self._random_position()\n",
    "        while self.map[self.agent_pos] != 0:\n",
    "            self.agent_pos = self._random_position()\n",
    "        \n",
    "        # 初始化代理方向和分数\n",
    "        self.agent_direction = 0  # 0:上, 1:左, 2:右, 3:下\n",
    "        self.score = INITIAL_SCORE  # 添加分数系统\n",
    "        \n",
    "        self.prev_agent_pos = self.agent_pos\n",
    "        self.steps = 0\n",
    "        self.terminated = False\n",
    "        self.truncated = False\n",
    "\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action_probs: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:\n",
    "        # 1. 计算移动概率分布\n",
    "        move_probs_dist = self._calculate_movement_distribution(action_probs)\n",
    "        \n",
    "        # 2. 选择移动方向\n",
    "        chosen_direction_idx = np.random.choice(NUM_VIEWS, p=move_probs_dist)\n",
    "        \n",
    "        # 3. 移动代理并更新方向\n",
    "        self.prev_agent_pos = self.agent_pos\n",
    "        self.agent_pos = self._move_in_direction(self.agent_pos, chosen_direction_idx)\n",
    "        self.agent_direction = chosen_direction_idx  # 移动后更新方向\n",
    "        \n",
    "        # 4. 处理交互和奖励\n",
    "        reward = 0\n",
    "        collided_entity_type = self.map[self.agent_pos]\n",
    "        \n",
    "        # 处理不同类型的碰撞\n",
    "        if collided_entity_type == 1:  # 捕食者\n",
    "            reward += REWARDS['predator']\n",
    "            self.terminated = True\n",
    "        elif collided_entity_type == 2:  # 食物\n",
    "            if self.agent_pos in self.foods:\n",
    "                reward += REWARDS['food']\n",
    "                # 立即删除食物并变为环境\n",
    "                del self.foods[self.agent_pos]\n",
    "                self.map[self.agent_pos] = 0\n",
    "        elif collided_entity_type == 0:  # 环境\n",
    "            reward += REWARDS['environment']\n",
    "        else:\n",
    "            reward += REWARDS['environment']\n",
    "        \n",
    "        # 5. 更新分数并检查游戏结束条件\n",
    "        self.score += reward\n",
    "        if self.score < 1:\n",
    "            self.terminated = True\n",
    "        \n",
    "        # 6. 移动捕食者\n",
    "        self._move_predators()\n",
    "        \n",
    "        # 7. 检查捕食者是否移动到代理位置\n",
    "        for pred in self.predators:\n",
    "            if pred['pos'] == self.agent_pos:\n",
    "                reward += REWARDS['predator']  # 确保即使代理移动后被捕食者追上也会受到惩罚\n",
    "                self.terminated = True\n",
    "                break\n",
    "        \n",
    "        # 8. 步数限制\n",
    "        self.steps += 1\n",
    "        if self.steps >= 1000:\n",
    "            self.truncated = True\n",
    "        \n",
    "        return self._get_observation(), reward, self.terminated, self.truncated, {}\n",
    "\n",
    "    def _get_direction_vectors(self) -> List[Tuple[int, int]]:\n",
    "        \"\"\"获取方向向量：前、左、右、后（相对于当前方向）\"\"\"\n",
    "        # 绝对方向：0:上, 1:左, 2:右, 3:下\n",
    "        # 根据代理当前方向计算相对方向\n",
    "        if self.agent_direction == 0:  # 面向\"上\"\n",
    "            return [(-1, 0), (0, -1), (0, 1), (1, 0)]  # 前、左、右、后\n",
    "        elif self.agent_direction == 1:  # 面向\"左\"\n",
    "            return [(0, -1), (1, 0), (-1, 0), (0, 1)]  # 前、左、右、后\n",
    "        elif self.agent_direction == 2:  # 面向\"右\"\n",
    "            return [(0, 1), (-1, 0), (1, 0), (0, -1)]  # 前、左、右、后\n",
    "        else:  # 面向\"下\"\n",
    "            return [(1, 0), (0, 1), (0, -1), (-1, 0)]  # 前、左、右、后\n",
    "\n",
    "    def _get_observation(self) -> np.ndarray:\n",
    "        \"\"\"获取代理四个方向的观察图像（基于当前方向）\"\"\"\n",
    "        observations = []\n",
    "        direction_vectors = self._get_direction_vectors()\n",
    "        \n",
    "        for dr, dc in direction_vectors:\n",
    "            target_pos = (\n",
    "                (self.agent_pos[0] + dr) % MAP_SIZE,\n",
    "                (self.agent_pos[1] + dc) % MAP_SIZE\n",
    "            )\n",
    "            entity_type = self.map[target_pos]\n",
    "            \n",
    "            if entity_type == 1:  # 捕食者\n",
    "                # 查找对应捕食者的图像\n",
    "                img_array = next(\n",
    "                    (self.image_datasets['predator'][p['img_idx']] \n",
    "                    for p in self.predators if p['pos'] == target_pos),\n",
    "                    np.random.choice(self.image_datasets['predator'])\n",
    "                )\n",
    "            elif entity_type == 2:  # 食物\n",
    "                img_array = self.image_datasets['food'][self.foods.get(target_pos, {}).get('img_idx', 0)]\n",
    "            else:  # 环境\n",
    "                img_array = np.random.choice(self.image_datasets['environment'])\n",
    "            \n",
    "            observations.append(img_array)\n",
    "        \n",
    "        return np.array(observations, dtype=np.uint8)\n",
    "\n",
    "    def _calculate_movement_distribution(self, action_probs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"计算移动概率分布（基于相对方向）\"\"\"\n",
    "        # 获取周围实体\n",
    "        surrounding_entities = []\n",
    "        direction_vectors = self._get_direction_vectors()\n",
    "        \n",
    "        for dr, dc in direction_vectors:\n",
    "            pos = (\n",
    "                (self.agent_pos[0] + dr) % MAP_SIZE,\n",
    "                (self.agent_pos[1] + dc) % MAP_SIZE\n",
    "            )\n",
    "            surrounding_entities.append(self.map[pos])\n",
    "        \n",
    "        move_dist = np.zeros(NUM_VIEWS)\n",
    "        \n",
    "        # Escape逻辑：远离感知到的捕食者（向相反方向移动）\n",
    "        predator_sensed = False\n",
    "        for i, entity in enumerate(surrounding_entities):\n",
    "            if entity == 1:  # 捕食者\n",
    "                # 相反方向是索引3-i（前<->后，左<->右）\n",
    "                opposite_idx = 3 - i\n",
    "                move_dist[opposite_idx] += action_probs[0]\n",
    "                predator_sensed = True\n",
    "        \n",
    "        if not predator_sensed:\n",
    "            move_dist += action_probs[0] / NUM_VIEWS\n",
    "        \n",
    "        # Eat逻辑：朝向感知到的食物\n",
    "        food_sensed = False\n",
    "        for i, entity in enumerate(surrounding_entities):\n",
    "            if entity == 2:  # 食物\n",
    "                move_dist[i] += action_probs[1]\n",
    "                food_sensed = True\n",
    "        \n",
    "        if not food_sensed:\n",
    "            move_dist += action_probs[1] / NUM_VIEWS\n",
    "        \n",
    "        # Wander逻辑：随机移动\n",
    "        move_dist += action_probs[2] / NUM_VIEWS\n",
    "        \n",
    "        # 归一化\n",
    "        total = np.sum(move_dist)\n",
    "        if total > 0:\n",
    "            move_dist /= total\n",
    "        else:\n",
    "            move_dist = np.ones(NUM_VIEWS) / NUM_VIEWS\n",
    "        \n",
    "        return move_dist\n",
    "\n",
    "    def _move_in_direction(self, current_pos: Tuple[int, int], direction_idx: int) -> Tuple[int, int]:\n",
    "        \"\"\"在指定方向上移动位置（基于相对方向）\"\"\"\n",
    "        dr, dc = self._get_direction_vectors()[direction_idx]\n",
    "        new_r = (current_pos[0] + dr) % MAP_SIZE\n",
    "        new_c = (current_pos[1] + dc) % MAP_SIZE\n",
    "        return new_r, new_c\n",
    "\n",
    "    def _move_predators(self):\n",
    "        \"\"\"改进的捕食者移动逻辑，解决移动冲突且禁止移动到食物位置\"\"\"\n",
    "        # 清除所有捕食者位置标记（临时设为环境）\n",
    "        for pred in self.predators:\n",
    "            self.map[pred['pos']] = 0\n",
    "        \n",
    "        moved_positions = set()  # 记录所有计划移动到的位置\n",
    "        new_predators = []       # 存储移动后的捕食者\n",
    "        move_vectors = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # 上、下、左、右\n",
    "        \n",
    "        # 随机顺序处理捕食者，确保公平性\n",
    "        random.shuffle(self.predators)\n",
    "        \n",
    "        for pred in self.predators:\n",
    "            current_pos = pred['pos']\n",
    "            moved = False\n",
    "            \n",
    "            # 尝试所有可能的移动方向（随机顺序）\n",
    "            random.shuffle(move_vectors)\n",
    "            \n",
    "            for dr, dc in move_vectors:\n",
    "                new_pos = (current_pos[0] + dr, current_pos[1] + dc)\n",
    "                \n",
    "                # 检查是否移出边界\n",
    "                if not (0 <= new_pos[0] < MAP_SIZE and 0 <= new_pos[1] < MAP_SIZE):\n",
    "                    # 在边缘生成新捕食者\n",
    "                    edge_pos = self._random_edge_position()\n",
    "                    \n",
    "                    # 确保新位置有效且未被占用\n",
    "                    if self.map[edge_pos] == 0 and edge_pos not in moved_positions:\n",
    "                        new_pred = {\n",
    "                            'pos': edge_pos,\n",
    "                            'img_idx': np.random.randint(len(self.image_datasets['predator']))\n",
    "                        }\n",
    "                        self.map[edge_pos] = 1\n",
    "                        moved_positions.add(edge_pos)\n",
    "                        new_predators.append(new_pred)\n",
    "                        moved = True\n",
    "                        break\n",
    "                    # 如果位置无效，继续尝试其他方向\n",
    "                    continue\n",
    "                \n",
    "                # 检查目标位置是否有效（只能是环境或代理，不能是食物或其他捕食者）\n",
    "                if self.map[new_pos] != 1 and self.map[new_pos] != 2 and new_pos not in moved_positions:\n",
    "                    # 检查是否与食物或其他捕食者冲突\n",
    "                    pred['pos'] = new_pos\n",
    "                    self.map[new_pos] = 1\n",
    "                    moved_positions.add(new_pos)\n",
    "                    new_predators.append(pred)\n",
    "                    moved = True\n",
    "                    break\n",
    "            \n",
    "            # 如果无法移动，留在原位（检查位置是否仍可用）\n",
    "            if not moved:\n",
    "                if current_pos not in moved_positions and self.map[current_pos] != 2:  # 不是食物\n",
    "                    self.map[current_pos] = 1\n",
    "                    moved_positions.add(current_pos)\n",
    "                    new_predators.append(pred)\n",
    "                else:\n",
    "                    # 当前位置已被占用，在随机位置生成新捕食者\n",
    "                    new_pos = self._random_position()\n",
    "                    if self.map[new_pos] == 0 and new_pos not in moved_positions:\n",
    "                        new_pred = {\n",
    "                            'pos': new_pos,\n",
    "                            'img_idx': np.random.randint(len(self.image_datasets['predator']))\n",
    "                        }\n",
    "                        self.map[new_pos] = 1\n",
    "                        moved_positions.add(new_pos)\n",
    "                        new_predators.append(new_pred)\n",
    "        \n",
    "        self.predators = new_predators\n",
    "\n",
    "    def _random_position(self) -> Tuple[int, int]:\n",
    "        \"\"\"随机位置（地图内）\"\"\"\n",
    "        return (np.random.randint(0, MAP_SIZE), np.random.randint(0, MAP_SIZE))\n",
    "\n",
    "    def _random_edge_position(self) -> Tuple[int, int]:\n",
    "        \"\"\"随机边缘位置\"\"\"\n",
    "        edge = np.random.choice(['top', 'bottom', 'left', 'right'])\n",
    "        if edge == 'top': \n",
    "            return (0, np.random.randint(0, MAP_SIZE))\n",
    "        if edge == 'bottom': \n",
    "            return (MAP_SIZE - 1, np.random.randint(0, MAP_SIZE))\n",
    "        if edge == 'left': \n",
    "            return (np.random.randint(0, MAP_SIZE), 0)\n",
    "        return (np.random.randint(0, MAP_SIZE), MAP_SIZE - 1)  # right\n",
    "\n",
    "    def _is_valid_position(self, pos: Tuple[int, int]) -> bool:\n",
    "        \"\"\"检查位置是否有效\"\"\"\n",
    "        return 0 <= pos[0] < MAP_SIZE and 0 <= pos[1] < MAP_SIZE\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"渲染当前地图状态\"\"\"\n",
    "        if self.current_map_image is None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 10))\n",
    "            self.current_map_image = ax.imshow(\n",
    "                self.map, \n",
    "                cmap='viridis', \n",
    "                vmin=0, \n",
    "                vmax=len(PIXEL_TYPES)-1\n",
    "            )\n",
    "            plt.colorbar(self.current_map_image, label='Entity Types')\n",
    "            plt.title(\"Survival Game Environment\")\n",
    "            plt.axis('off')\n",
    "            plt.ion()\n",
    "            plt.show()\n",
    "        \n",
    "        # 创建带代理标记的地图\n",
    "        display_map = self.map.copy()\n",
    "        display_map[self.agent_pos] = 3  # 标记代理位置\n",
    "        \n",
    "        self.current_map_image.set_data(display_map)\n",
    "        plt.title(f\"Step: {self.steps}, Agent Position: {self.agent_pos}, Score: {self.score:.1f}\")\n",
    "        plt.gcf().canvas.draw_idle()\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"关闭渲染\"\"\"\n",
    "        if self.current_map_image is not None:\n",
    "            plt.ioff()\n",
    "            plt.close()\n",
    "            self.current_map_image = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8f5316",
   "metadata": {},
   "source": [
    "# 2. 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3afaa4",
   "metadata": {},
   "source": [
    "## 2.1 模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de989b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# Model Architecture (Wrappers for Perception)\n",
    "# ========================\n",
    "class PerceptionModule(nn.Module):\n",
    "    def __init__(self, pretrained_convnext=True):\n",
    "        super(PerceptionModule, self).__init__()\n",
    "        self.convnext = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.DEFAULT if pretrained_convnext else None)\n",
    "        self.convnext.classifier = nn.Identity() # Remove original classifier\n",
    "        \n",
    "        self.do_normalize = pretrained_convnext\n",
    "        if self.do_normalize:\n",
    "            self.normalize_transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x is (B, N_VIEWS, C, H, W), scaled [0,1]\n",
    "        \n",
    "        # Hooks for visualization (GradCAM needs these on a specific layer, see ModelVisualizer)\n",
    "        # For general input/output grads if needed:\n",
    "        # if x.requires_grad:\n",
    "        #     x.register_hook(lambda grad: setattr(self, 'input_gradients', grad))\n",
    "        \n",
    "        batch_size, num_views, C, H, W = x.size()\n",
    "        x_input_to_convnext = x.view(batch_size * num_views, C, H, W)\n",
    "\n",
    "        if self.do_normalize:\n",
    "            x_input_to_convnext = self.normalize_transform(x_input_to_convnext)\n",
    "        \n",
    "        # Register hook for GradCAM on the output of the target layer if not done externally\n",
    "        # features = self.convnext(x_input_to_convnext)\n",
    "        # For GradCAM, typically want features from a specific conv layer, not the final output of convnext here.\n",
    "        # The ModelVisualizer will handle hooking the specific internal layer.\n",
    "        \n",
    "        raw_features = self.convnext(x_input_to_convnext) # Output: (B*N_VIEWS, feature_dim e.g. 768)\n",
    "        return raw_features.view(batch_size, num_views, -1) # (B, N_VIEWS, feature_dim)\n",
    "\n",
    "class PretrainedEncoderWrapper(nn.Module):\n",
    "    def __init__(self, encoder_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder_model # This is a ConvNeXt model (autoencoder.encoder)\n",
    "        # AE encoder was trained on [0,1] images, so no ImageNet normalization here.\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x is (batch_size, num_views, C, H, W), already scaled to [0,1]\n",
    "        batch_size, num_views, C, H, W = x.size()\n",
    "        x_reshaped = x.view(batch_size * num_views, C, H, W)\n",
    "        \n",
    "        features = self.encoder(x_reshaped) # encoder is ConvNeXt, outputs (B*N_VIEWS, 768)\n",
    "        return features.view(batch_size, num_views, -1)\n",
    "\n",
    "# Decision module as defined in doc (used by ModelVisualizer, SB3 PPO has its own MLP head)\n",
    "class StandaloneDecisionModule(nn.Module):\n",
    "    def __init__(self, input_dim: int = NUM_VIEWS * 768, hidden_dims: List[int] = [512, 256]):\n",
    "        super(StandaloneDecisionModule, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, 3)) # Output 3 action probabilities\n",
    "        layers.append(nn.Softmax(dim=-1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x is (B, N_VIEWS * feature_dim)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# 自编码器\n",
    "# ========================\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.DEFAULT) # Or None for from scratch\n",
    "        self.encoder.classifier = nn.Identity() # Output of encoder is (B, 768)\n",
    "\n",
    "        # Decoder: (B, 768, 1, 1) -> (B, 3, 100, 100)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(768, 512, kernel_size=5, stride=1, padding=0), # (B, 512, 5, 5)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=5, padding=0), # (B, 256, 25, 25)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # (B, 128, 50, 50)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # (B, 64, 100, 100)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=3, stride=1, padding=1),   # (B, 3, 100, 100)\n",
    "            nn.Sigmoid() # Output images in [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # x is (B, 3, H, W)\n",
    "        encoded_flat = self.encoder(x) # (B, 768)\n",
    "        encoded_reshaped = encoded_flat.view(-1, 768, 1, 1) # Reshape for ConvTranspose\n",
    "        decoded = self.decoder(encoded_reshaped)\n",
    "        return decoded\n",
    "\n",
    "# ========================\n",
    "# 特征提取器 for Stable Baselines3\n",
    "# ========================\n",
    "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, perception_module: nn.Module):\n",
    "        # features_dim is N_VIEWS * perception_module_output_dim_per_view\n",
    "        # Assuming perception_module outputs 768 features per view\n",
    "        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim=NUM_VIEWS * 768)\n",
    "        self.perception = perception_module # This is PerceptionModule or PretrainedEncoderWrapper\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        # Input: (batch, NUM_VIEWS, H, W, C) uint8\n",
    "        # Permute to (batch, NUM_VIEWS, C, H, W) and scale to [0,1]\n",
    "        observations_processed = observations.permute(0, 1, 4, 2, 3).float() / 255.0\n",
    "        \n",
    "        # self.perception module (PerceptionModule or PretrainedEncoderWrapper)\n",
    "        # expects (B, N_VIEWS, C, H, W) scaled [0,1]\n",
    "        # and returns (B, N_VIEWS, feature_dim_per_view)\n",
    "        features_per_view = self.perception(observations_processed)\n",
    "        \n",
    "        # Flatten features from all views: (B, N_VIEWS * feature_dim_per_view)\n",
    "        return features_per_view.reshape(observations.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c1a2e",
   "metadata": {},
   "source": [
    "## 2.2 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533eb225",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# 训练方法\n",
    "# ========================\n",
    "def fitness_training(env: gym.Env, total_timesteps: int = 10000):\n",
    "    # PerceptionModule handles its own normalization if pretrained\n",
    "    perception_fitness = PerceptionModule(pretrained_convnext=True) \n",
    "    \n",
    "    policy_kwargs = dict(\n",
    "        features_extractor_class=CustomFeatureExtractor,\n",
    "        features_extractor_kwargs=dict(perception_module=perception_fitness),\n",
    "        net_arch=[dict(pi=[256, 128], vf=[256, 128])], # MLP layers for PPO policy and value\n",
    "    )\n",
    "    \n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, device=\"auto\")\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    return model\n",
    "\n",
    "def truth_training(env: gym.Env, ae_path: str, total_timesteps: int = 10000):\n",
    "    autoencoder = Autoencoder()\n",
    "    autoencoder.load_state_dict(torch.load(ae_path))\n",
    "    ae_encoder = autoencoder.encoder\n",
    "    \n",
    "    for param in ae_encoder.parameters(): # Freeze encoder parameters\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Wrap the frozen AE encoder to match the perception module interface\n",
    "    perception_truth = PretrainedEncoderWrapper(ae_encoder)\n",
    "    \n",
    "    policy_kwargs = dict(\n",
    "        features_extractor_class=CustomFeatureExtractor,\n",
    "        features_extractor_kwargs=dict(perception_module=perception_truth),\n",
    "        net_arch=[dict(pi=[256, 128], vf=[256, 128])]\n",
    "    )\n",
    "    \n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, device=\"auto\")\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    return model\n",
    "\n",
    "def train_autoencoder(image_datasets: Dict[str, List[np.ndarray]],\n",
    "                      epochs: int = 10,\n",
    "                      batch_size: int = 32,\n",
    "                      ae_save_path: str = \"autoencoder.pth\",\n",
    "                      num_workers: int = 0 # For DataLoader: 0 means data loaded in main process\n",
    "                     ) -> Autoencoder: # Assuming Autoencoder class is defined\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Autoencoder().to(device) # Ensure Autoencoder class is defined\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Combine all images for training from the input dictionary\n",
    "    all_images_np = [img for cat_imgs in image_datasets.values() for img in cat_imgs]\n",
    "\n",
    "    if not all_images_np:\n",
    "        print(\"No images found to train the autoencoder. Aborting.\")\n",
    "        return model # Or raise an error\n",
    "\n",
    "    # Define the transformation - ToTensor is crucial\n",
    "    # It converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255]\n",
    "    # to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Create the custom dataset\n",
    "    custom_dataset = CustomImageDataset(image_list=all_images_np, transform=transform)\n",
    "\n",
    "    # Create the DataLoader\n",
    "    # The DataLoader will now handle batching and shuffling efficiently.\n",
    "    # Images are loaded and transformed on-the-fly (or by worker processes if num_workers > 0)\n",
    "    # and only a batch at a time is moved to the GPU.\n",
    "    dataloader = DataLoader(custom_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=num_workers, # Adjust based on your system\n",
    "                            pin_memory=True if device.type == 'cuda' else False) # Speeds up CPU to GPU transfer\n",
    "\n",
    "    print(f\"Training Autoencoder with {len(custom_dataset)} images on {device}...\")\n",
    "    print(f\"Number of batches per epoch: {len(dataloader)}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, inputs in enumerate(dataloader): # inputs are (B, C, H, W)\n",
    "            # Move the batch of inputs to the designated device\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs) # Autoencoder reconstructs the input\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if (i + 1) % (max(1, len(dataloader) // 10)) == 0: # Print progress 10 times per epoch\n",
    "                print(f\"  Epoch {epoch+1}/{epochs}, Batch {i+1}/{len(dataloader)}, \"\n",
    "                      f\"Current Avg Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "        epoch_loss = running_loss / len(custom_dataset)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} finished. Average Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), ae_save_path)\n",
    "    print(f\"Autoencoder trained and saved to {ae_save_path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176dafac",
   "metadata": {},
   "source": [
    "# 3. 评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf844a",
   "metadata": {},
   "source": [
    "## 3.1 Survival Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c07c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# 生存评估函数\n",
    "# ========================\n",
    "def evaluate_survival(model: PPO, env: gym.Env, n_episodes: int = 10) -> float:\n",
    "    total_steps = 0\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        episode_steps = 0\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            # 获取原始动作概率分布\n",
    "            with torch.no_grad():\n",
    "                # 1. 将 NumPy observation 转换为 PyTorch 张量\n",
    "                #    model.policy.get_distribution 期望的输入是已经经过特征提取器处理的潜在特征，\n",
    "                #    或者如果直接传入观测值，它内部会调用特征提取器。\n",
    "                #    为确保正确处理，需要将 obs 转换为 tensor 并添加 batch 维度。\n",
    "                obs_tensor = torch.as_tensor(obs, device=model.device).unsqueeze(0)\n",
    "                dist = model.policy.get_distribution(obs_tensor)\n",
    "                # dist.probs 通常是 (batch_size, num_actions)\n",
    "                action_probs = dist.distribution.probs.cpu().numpy().squeeze() # 使用 .squeeze() 移除 batch 维度\n",
    "\n",
    "            # 环境交互（传入概率分布）\n",
    "            obs, _, terminated, truncated, _ = env.step(action_probs)\n",
    "            episode_steps += 1\n",
    "\n",
    "        total_steps += episode_steps\n",
    "\n",
    "    return total_steps / n_episodes\n",
    "\n",
    "def evaluate_survival_with_render(model: PPO, env: gym.Env, n_episodes: int = 1) -> float:\n",
    "    total_steps = 0\n",
    "    if n_episodes <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        episode_steps = 0\n",
    "        print(f\"Starting Episode {episode + 1} with rendering...\")\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            env.render()  # <-- 调用渲染\n",
    "\n",
    "            # 获取原始动作概率分布\n",
    "            with torch.no_grad():\n",
    "                # 将 NumPy observation 转换为 PyTorch 张量, 添加 batch 维度，并放到正确的 device\n",
    "                # obs from env is (NUM_VIEWS, H, W, C) numpy.uint8\n",
    "                obs_tensor = torch.as_tensor(obs, device=model.device).unsqueeze(0)\n",
    "                # CustomFeatureExtractor 会处理 permute 和 scaling\n",
    "                # get_distribution 会在内部使用特征提取器处理 obs_tensor\n",
    "\n",
    "                distribution = model.policy.get_distribution(obs_tensor)\n",
    "                # 对于 CategoricalDistribution (离散动作空间), 概率在 distribution.probs\n",
    "                # .squeeze() 移除批处理维度，得到 (num_actions,) 的概率分布\n",
    "                action_probs = distribution.distribution.probs.cpu().numpy().squeeze()\n",
    "\n",
    "            # 环境交互（传入概率分布）\n",
    "            obs, reward, terminated, truncated, _ = env.step(action_probs)\n",
    "            episode_steps += 1\n",
    "\n",
    "            # 可选：在 episode 内部打印一些信息\n",
    "            if episode_steps % 50 == 0:\n",
    "                print(f\"  Episode {episode + 1}, Step {episode_steps}, Current Score: {env.score:.1f}\")\n",
    "\n",
    "        total_steps += episode_steps\n",
    "        print(f\"Episode {episode + 1} finished after {episode_steps} steps. Final Score: {env.score:.1f}\")\n",
    "\n",
    "    env.close()  # 在所有 episodes 结束后关闭渲染窗口\n",
    "    return total_steps / n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ffa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# 可视化评估工具\n",
    "# ========================\n",
    "class ModelVisualizer:\n",
    "    def __init__(self, perception_module: nn.Module, decision_module: nn.Module):\n",
    "        self.perception = perception_module\n",
    "        self.decision = decision_module # PPO's policy_net (mlp_extractor.policy_net)\n",
    "        \n",
    "        # --- Hooks and stored data management ---\n",
    "        # For GradCAM specific target layer\n",
    "        self.target_layer_hook_activations: Optional[torch.Tensor] = None\n",
    "        self.target_layer_hook_gradients: Optional[torch.Tensor] = None\n",
    "        \n",
    "        # For VBP/GuidedBP (and general hook management)\n",
    "        self._hook_handles: List[torch.utils.hooks.RemovableHandle] = []\n",
    "        self.module_to_forward_output: Dict[nn.Module, torch.Tensor] = {}\n",
    "\n",
    "    def cleanup_hooks(self):\n",
    "        \"\"\"Removes all registered hooks and clears stored hook-related data.\"\"\"\n",
    "        for handle in self._hook_handles:\n",
    "            handle.remove()\n",
    "        self._hook_handles = []\n",
    "        \n",
    "        self.target_layer_hook_activations = None\n",
    "        self.target_layer_hook_gradients = None\n",
    "        self.module_to_forward_output = {}\n",
    "\n",
    "    # --- Hook callback functions ---\n",
    "    def _store_forward_output_hook(self, module: nn.Module, input_val: Any, output_val: torch.Tensor):\n",
    "        \"\"\"Stores the output of a module during forward pass (for VBP).\"\"\"\n",
    "        self.module_to_forward_output[module] = output_val.detach()\n",
    "\n",
    "    def _gelu_hook_function_vbp(self, module: nn.Module, grad_input: Tuple[torch.Tensor, ...], grad_output: Tuple[torch.Tensor, ...]) -> Optional[Tuple[torch.Tensor, ...]]:\n",
    "        \"\"\"Approximated Guided Backpropagation hook for GELU.\"\"\"\n",
    "        if module not in self.module_to_forward_output:\n",
    "            # This can happen if forward pass didn't go through this specific module\n",
    "            # or if hooks were not registered correctly for the forward pass.\n",
    "            return None # Or grad_input if we want to allow passthrough\n",
    "\n",
    "        corresponding_forward_output = self.module_to_forward_output[module]\n",
    "        # Guided BP logic: only pass gradient if grad_output is positive AND forward output was positive\n",
    "        # grad_output[0] is the gradient w.r.t. the module's output.\n",
    "        # torch.clamp(grad_output[0], min=0.0) ensures only positive gradients from above are considered.\n",
    "        # (corresponding_forward_output > 0).float() ensures neuron was active.\n",
    "        guided_grad = torch.clamp(grad_output[0], min=0.0) * (corresponding_forward_output > 0).float()\n",
    "        return (guided_grad,)\n",
    "\n",
    "    def _relu_hook_function_vbp(self, module: nn.Module, grad_input: Tuple[torch.Tensor, ...], grad_output: Tuple[torch.Tensor, ...]) -> Optional[Tuple[torch.Tensor, ...]]:\n",
    "        \"\"\"Guided Backpropagation hook for ReLU.\"\"\"\n",
    "        if module not in self.module_to_forward_output:\n",
    "            return None\n",
    "            \n",
    "        corresponding_forward_output = self.module_to_forward_output[module]\n",
    "        guided_grad = torch.clamp(grad_output[0], min=0.0) * (corresponding_forward_output > 0).float()\n",
    "        return (guided_grad,)\n",
    "\n",
    "    def _gradcam_activation_hook(self, module: nn.Module, input_val: Any, output_val: torch.Tensor):\n",
    "        \"\"\"Stores activations for GradCAM.\"\"\"\n",
    "        self.target_layer_hook_activations = output_val.detach()\n",
    "\n",
    "    def _gradcam_gradient_hook(self, module: nn.Module, grad_input: Any, grad_output: Tuple[torch.Tensor, ...]):\n",
    "        \"\"\"Stores gradients for GradCAM.\"\"\"\n",
    "        self.target_layer_hook_gradients = grad_output[0].detach()\n",
    "\n",
    "    # --- Main Visualization Methods ---\n",
    "    def _register_gradcam_hooks(self):\n",
    "        \"\"\"Registers hooks for GradCAM on the target ConvNeXt layer.\"\"\"\n",
    "        self.cleanup_hooks() # Clear any previous hooks\n",
    "\n",
    "        convnext_model = None\n",
    "        if hasattr(self.perception, 'convnext'): # For PerceptionModule\n",
    "            convnext_model = self.perception.convnext\n",
    "        elif hasattr(self.perception, 'encoder'): # For PretrainedEncoderWrapper\n",
    "            convnext_model = self.perception.encoder\n",
    "        else:\n",
    "            raise TypeError(\"Perception module is of an unknown type for GradCAM hook registration.\")\n",
    "\n",
    "        try:\n",
    "            # Target the output of the last stage in ConvNeXt features\n",
    "            target_layer = convnext_model.features[-1] \n",
    "            handle_fwd = target_layer.register_forward_hook(self._gradcam_activation_hook)\n",
    "            handle_bwd = target_layer.register_full_backward_hook(self._gradcam_gradient_hook)\n",
    "            self._hook_handles.extend([handle_fwd, handle_bwd])\n",
    "        except Exception as e:\n",
    "            print(f\"Error registering GradCAM hooks: {e}. GradCAM might not work correctly.\")\n",
    "            self.cleanup_hooks() # Ensure partial hooks are removed\n",
    "    \n",
    "    def activation_maximization(self, action_idx: int, lr: float = 0.1, steps: int = 200, num_views_for_am = NUM_VIEWS) -> np.ndarray:\n",
    "        self.perception.eval()\n",
    "        self.decision.eval()\n",
    "        print(f\"Starting AM for action {action_idx}...\")\n",
    "\n",
    "        # Optimizes a single (1,3,H,W) image, assuming it's one of the N_VIEWS inputs\n",
    "        # The perception module will process it as (1, 1, C, H, W) effectively\n",
    "        # Then its features are replicated for the decision module.\n",
    "        optimized_image_tensor = torch.rand(1, 3, 100, 100, requires_grad=True, device=next(self.perception.parameters()).device)\n",
    "        optimizer = optim.Adam([optimized_image_tensor], lr=lr, weight_decay=1e-4)\n",
    "\n",
    "        for i in range(steps):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Clamp and ensure image is in [0,1] range for perception module\n",
    "            current_image_0_1 = torch.clamp(optimized_image_tensor, 0.0, 1.0)\n",
    "            \n",
    "            # Perception module expects (B, N_VIEWS, C, H, W)\n",
    "            # We form an input where one view is the optimized image, others could be neutral (e.g., gray)\n",
    "            # For simplicity in AM: assume the optimized image is so dominant it works if it's just one view.\n",
    "            # The self.perception here is the SB3 model's feature extractor's perception part.\n",
    "            # It expects (B, N_VIEWS, C, H, W) format.\n",
    "            # So, let's treat the optimized image as if it's all N_VIEWS for AM purposes.\n",
    "            multi_view_input = current_image_0_1.repeat(1, num_views_for_am, 1, 1, 1).squeeze(0) # (N_VIEWS, C, H, W)\n",
    "            multi_view_input = multi_view_input.unsqueeze(0) # (1, N_VIEWS, C, H, W)\n",
    "\n",
    "\n",
    "            # Get features from perception ( (1, N_VIEWS, feat_dim) )\n",
    "            features_per_view = self.perception(multi_view_input)\n",
    "            # Flatten for decision module ( (1, N_VIEWS * feat_dim) )\n",
    "            flat_features = features_per_view.view(1, -1)\n",
    "            \n",
    "            action_distribution = self.decision(flat_features) # decision is PPO's policy_net\n",
    "            \n",
    "            loss = -action_distribution[0, action_idx] # Maximize prob of this action\n",
    "            \n",
    "            # Add some regularization to the image (e.g., total variation)\n",
    "            loss += 0.0001 * torch.sum(torch.abs(current_image_0_1[:, :, :, :-1] - current_image_0_1[:, :, :, 1:])) + \\\n",
    "                    0.0001 * torch.sum(torch.abs(current_image_0_1[:, :, :-1, :] - current_image_0_1[:, :, 1:, :]))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % (steps // 10) == 0:\n",
    "                 print(f\"AM step {i}, loss {loss.item()}\")\n",
    "\n",
    "        final_image_0_1 = torch.clamp(optimized_image_tensor.detach(), 0.0, 1.0)\n",
    "        generated_np = final_image_0_1.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "        return (generated_np * 255).astype(np.uint8)\n",
    "\n",
    "    def grad_cam(self, obs_tensor_0_1: torch.Tensor, action_idx: int, target_view_idx: int = 0) -> Optional[np.ndarray]:\n",
    "        # obs_tensor_0_1 is (1, N_VIEWS, C, H, W), scaled [0,1]\n",
    "        # target_view_idx specifies which of the N_VIEWS to generate GradCAM for.\n",
    "        self.perception.eval()\n",
    "        self.decision.eval()\n",
    "        self._register_gradcam_hooks() # Ensure hooks are on the correct layer\n",
    "        \n",
    "        if not self._hook_handles: # Check if hooks were successfully registered\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        obs_tensor_0_1.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass\n",
    "        # self.perception is the perception module from the SB3 agent (e.g., PerceptionModule or PretrainedEncoderWrapper)\n",
    "        features_per_view = self.perception(obs_tensor_0_1) # (1, N_VIEWS, feat_dim)\n",
    "        flat_features = features_per_view.view(1, -1)       # (1, N_VIEWS * feat_dim)\n",
    "        \n",
    "        # self.decision is the PPO's policy_net\n",
    "        action_distribution = self.decision(flat_features) # (1, num_actions)\n",
    "        \n",
    "        # Backward pass for the target action\n",
    "        self.perception.zero_grad() # Zero grads for perception module's ConvNeXt\n",
    "        if self.decision.parameters(): # Also zero grads for decision MLP if it has params\n",
    "            self.decision.zero_grad()\n",
    "\n",
    "        score = action_distribution[0, action_idx]\n",
    "        score.backward(retain_graph=True)\n",
    "\n",
    "        if self.target_layer_hook_activations is None or self.target_layer_hook_gradients is None:\n",
    "            print(\"GradCAM: Activations or gradients not captured. Hooks might not be set correctly.\")\n",
    "            return None\n",
    "\n",
    "        # Activations/Gradients are from the ConvNeXt internal layer, shape (N_VIEWS_eff, C_feat, H_feat, W_feat)\n",
    "        # N_VIEWS_eff is batch_size * num_views from the perception module's internal reshaping. Here batch_size=1.\n",
    "        activations_all_views = self.target_layer_hook_activations # (NUM_VIEWS, C_feat, H_feat, W_feat)\n",
    "        gradients_all_views = self.target_layer_hook_gradients     # (NUM_VIEWS, C_feat, H_feat, W_feat)\n",
    "\n",
    "        # Select the specific view\n",
    "        activations_target_view = activations_all_views[target_view_idx] # (C_feat, H_feat, W_feat)\n",
    "        gradients_target_view = gradients_all_views[target_view_idx]   # (C_feat, H_feat, W_feat)\n",
    "        \n",
    "        # Compute weights (alpha_k)\n",
    "        pooled_gradients = torch.mean(gradients_target_view, dim=[1, 2]) # (C_feat)\n",
    "        \n",
    "        # Weight activations\n",
    "        for i in range(activations_target_view.shape[0]): # Loop over channels\n",
    "            activations_target_view[i, :, :] *= pooled_gradients[i]\n",
    "            \n",
    "        heatmap = torch.mean(activations_target_view, dim=0).cpu().numpy() # (H_feat, W_feat)\n",
    "        heatmap = np.maximum(heatmap, 0) # ReLU\n",
    "        if np.max(heatmap) > 0:\n",
    "            heatmap /= np.max(heatmap) # Normalize\n",
    "        \n",
    "        # Resize to original image size\n",
    "        original_h, original_w = obs_tensor_0_1.shape[-2:]\n",
    "        heatmap_resized = cv2.resize(heatmap, (original_w, original_h))\n",
    "        return heatmap_resized\n",
    "    \n",
    "    def visual_back_prop(self, obs_tensor_0_1: torch.Tensor, target_view_idx: int = 0) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generates a VisualBackProp (Guided Backpropagation style) saliency map.\n",
    "        Shows general input patterns contributing to the perception module's features for a view.\n",
    "        \"\"\"\n",
    "        self.perception.eval()\n",
    "        self.cleanup_hooks() # Clears all hooks and stored data (module_to_forward_output too)\n",
    "\n",
    "        convnext_model = None\n",
    "        if hasattr(self.perception, 'convnext'):\n",
    "            convnext_model = self.perception.convnext\n",
    "        elif hasattr(self.perception, 'encoder'):\n",
    "            convnext_model = self.perception.encoder\n",
    "        else:\n",
    "            print(\"VBP: Perception module type not recognized.\")\n",
    "            return None\n",
    "\n",
    "        # Register VBP hooks on all GELU/ReLU layers in the ConvNeXt model\n",
    "        for module_name, module in convnext_model.named_modules():\n",
    "            if isinstance(module, nn.GELU):\n",
    "                self._hook_handles.append(module.register_forward_hook(self._store_forward_output_hook))\n",
    "                self._hook_handles.append(module.register_full_backward_hook(self._gelu_hook_function_vbp))\n",
    "            elif isinstance(module, nn.ReLU): # Fallback for any ReLUs\n",
    "                self._hook_handles.append(module.register_forward_hook(self._store_forward_output_hook))\n",
    "                self._hook_handles.append(module.register_full_backward_hook(self._relu_hook_function_vbp))\n",
    "        \n",
    "        if not self._hook_handles:\n",
    "            print(\"VBP: No suitable activation layers (GELU/ReLU) found to hook in ConvNeXt.\")\n",
    "            return None # cleanup_hooks already called, so state is clean.\n",
    "\n",
    "        # Prepare input image: clone, detach, and set requires_grad\n",
    "        input_img_for_vbp = obs_tensor_0_1.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        # --- Forward pass ---\n",
    "        # This single forward pass will:\n",
    "        # 1. Populate self.module_to_forward_output via the forward hooks.\n",
    "        # 2. Give us features_per_view to backpropagate from.\n",
    "        features_per_view = self.perception(input_img_for_vbp) # Output: (B, N_VIEWS, feature_dim)\n",
    "        \n",
    "        # --- Backward pass ---\n",
    "        self.perception.zero_grad() # Zero gradients for the perception model parameters\n",
    "        if input_img_for_vbp.grad is not None:\n",
    "            input_img_for_vbp.grad.data.zero_()\n",
    "\n",
    "        # Target for backpropagation: Sum of features for the target_view_idx.\n",
    "        # This gives a \"general pattern\" for that view's features.\n",
    "        target_features_for_bp = features_per_view[0, target_view_idx, :] # Shape: (feature_dim)\n",
    "        \n",
    "        # Gradient for backward pass (sum of features -> gradient of 1 for each feature)\n",
    "        grad_outputs_for_bp = torch.ones_like(target_features_for_bp)\n",
    "        \n",
    "        target_features_for_bp.backward(gradient=grad_outputs_for_bp, retain_graph=False)\n",
    "\n",
    "        # --- Retrieve and process gradient on the input image ---\n",
    "        saliency_grad = input_img_for_vbp.grad\n",
    "        if saliency_grad is None:\n",
    "            print(\"VBP: Gradient for input image was not computed.\")\n",
    "            # Hooks will be cleaned up by the next call to a viz method or external `visualizer.cleanup_hooks()`\n",
    "            return None\n",
    "\n",
    "        # Take absolute value of gradients for the target view\n",
    "        saliency_abs = saliency_grad.data.abs() # Shape: (1, N_VIEWS, C, H, W)\n",
    "        saliency_target_view_abs = saliency_abs[0, target_view_idx, :, :, :] # Shape: (C, H, W)\n",
    "        \n",
    "        saliency_target_view_np = saliency_target_view_abs.cpu().numpy()\n",
    "        \n",
    "        # Normalize: max across channels, then scale to [0, 255]\n",
    "        saliency_map = np.max(saliency_target_view_np, axis=0) # Shape: (H, W)\n",
    "        if np.max(saliency_map) > 0:\n",
    "            saliency_map /= np.max(saliency_map)\n",
    "        saliency_map_uint8 = (saliency_map * 255).astype(np.uint8)\n",
    "        \n",
    "        # Hooks will be cleaned up by the next call to a viz method or an explicit call to self.cleanup_hooks().\n",
    "        return saliency_map_uint8\n",
    "    def cleanup_hooks(self):\n",
    "        for handle in self._hook_handles:\n",
    "            handle.remove()\n",
    "        self._hook_handles = []\n",
    "\n",
    "\n",
    "# ========================\n",
    "# 可视化评估函数\n",
    "# ========================\n",
    "def visualize_truth_evaluation(model: PPO, env: gym.Env, n_samples: int = 3, run_name=\"default\"):\n",
    "    print(f\"\\nPerforming Truth visualization evaluation for {run_name}...\")\n",
    "    \n",
    "    # Extract perception and decision modules from the PPO model\n",
    "    # The feature extractor itself contains the perception_module\n",
    "    feature_extractor = model.policy.features_extractor\n",
    "    if not hasattr(feature_extractor, 'perception'):\n",
    "        print(\"Error: PPO model's feature_extractor does not have a 'perception' attribute.\")\n",
    "        return\n",
    "    \n",
    "    perception_module_from_agent = feature_extractor.perception\n",
    "    # The decision module is the policy network part of the MLP extractor\n",
    "    decision_module_from_agent = model.policy.mlp_extractor.policy_net\n",
    "\n",
    "    visualizer = ModelVisualizer(perception_module_from_agent, decision_module_from_agent)\n",
    "    \n",
    "    action_names = ['Escape', 'Eat', 'Wander']\n",
    "\n",
    "    # 1. Activation Maximization\n",
    "    print(\"Generating Activation Maximization visualizations...\")\n",
    "    for action_idx, action_name in enumerate(action_names):\n",
    "        am_image_np = visualizer.activation_maximization(action_idx)\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(am_image_np)\n",
    "        plt.title(f'AM ({run_name}): {action_name}')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f'am_{run_name}_{action_name.lower()}.png')\n",
    "        plt.close()\n",
    "\n",
    "    # 2. Saliency Maps (GradCAM)\n",
    "    print(\"Generating Saliency Maps (GradCAM)...\")\n",
    "    obs_np, _ = env.reset() # obs_np is (N_VIEWS, H, W, C) uint8\n",
    "    for i in range(n_samples):\n",
    "        # Convert current observation to tensor, scale to [0,1]\n",
    "        # (N_VIEWS, H, W, C) -> (1, N_VIEWS, C, H, W)\n",
    "        obs_tensor_0_1 = torch.tensor(obs_np, dtype=torch.float32, device=model.device).permute(0, 3, 1, 2).unsqueeze(0) / 255.0\n",
    "        \n",
    "        # Get action from model (PPO model.predict expects the numpy obs)\n",
    "        action_idx_sb3, _ = model.predict(obs_np, deterministic=True)\n",
    "        action_name = action_names[action_idx_sb3]\n",
    "\n",
    "        # Generate GradCAM for the first view (e.g., \"front\" view)\n",
    "        target_view_for_gradcam = 0 # 0: Front, 1: Left, etc.\n",
    "        grad_cam_map = visualizer.grad_cam(obs_tensor_0_1, action_idx_sb3, target_view_idx=target_view_for_gradcam)\n",
    "        \n",
    "        input_image_to_show_np = obs_np[target_view_for_gradcam] # (H, W, C) uint8\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(input_image_to_show_np)\n",
    "        plt.title(f'Input View ({run_name}, Sample {i+1})\\nAction: {action_name}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if grad_cam_map is not None:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(input_image_to_show_np, alpha=0.7)\n",
    "            plt.imshow(grad_cam_map, cmap='jet', alpha=0.3)\n",
    "            plt.title('GradCAM')\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'saliency_{run_name}_sample_{i+1}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        action_probs_for_env = np.zeros(3)\n",
    "        action_probs_for_env[action_idx_sb3] = 1.0\n",
    "        obs_np, _, terminated, truncated, _ = env.step(action_probs_for_env)\n",
    "        if terminated or truncated:\n",
    "            obs_np, _ = env.reset()\n",
    "            if i + 1 >= n_samples: break # Avoid reset if last sample\n",
    "\n",
    "    # 3. VisualBackProp\n",
    "    print(\"Generating VisualBackProp visualizations...\")\n",
    "    # Get a fresh observation if needed, or reuse\n",
    "    # obs_np, _ = env.reset() # current obs_np is from end of GradCAM loop\n",
    "    \n",
    "    for i in range(n_samples): # Use n_samples or a different number for VBP\n",
    "        # Ensure obs_np is current for this iteration\n",
    "        if i > 0 or not ('obs_np' in locals() and obs_np is not None): # if not first iter or obs_np is not set\n",
    "             action_probs_dummy = np.array([0.0, 0.0, 1.0]) # e.g., Wander\n",
    "             obs_np, _, terminated, truncated, _ = env.step(action_probs_dummy)\n",
    "             if terminated or truncated:\n",
    "                 obs_np, _ = env.reset()\n",
    "                 if i + 1 >= n_samples: break # Avoid issues if last sample leads to reset\n",
    "\n",
    "        obs_tensor_vbp_0_1 = torch.tensor(obs_np, dtype=torch.float32, device=model.device).permute(0, 3, 1, 2).unsqueeze(0) / 255.0\n",
    "        \n",
    "        target_view_for_vbp = 0 # e.g., Front view\n",
    "        vbp_map = visualizer.visual_back_prop(obs_tensor_vbp_0_1, target_view_idx=target_view_for_vbp)\n",
    "        \n",
    "        input_image_np_vbp = obs_np[target_view_for_vbp] # (H, W, C) uint8 for plotting\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(input_image_np_vbp)\n",
    "        plt.title(f'Input View ({run_name}, VBP Sample {i+1})')\n",
    "        plt.axis('off')\n",
    "\n",
    "        if vbp_map is not None:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(vbp_map, cmap='gray') # VBP typically shown in grayscale\n",
    "            plt.title('VisualBackProp')\n",
    "            plt.axis('off')\n",
    "        else:\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.text(0.5, 0.5, \"VBP Failed\", ha='center', va='center')\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'vbp_{run_name}_sample_{i+1}.png')\n",
    "        # plt.show() # Optional: show plot interactively\n",
    "        plt.close()\n",
    "        \n",
    "        if i + 1 >= n_samples and (terminated or truncated): # Check if loop should break after reset\n",
    "            break\n",
    "\n",
    "    visualizer.cleanup_hooks() # Important to remove hooks after use\n",
    "    print(f\"Truth visualization for {run_name} complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395fe687",
   "metadata": {},
   "source": [
    "# 4. 主工作流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# 主工作流程\n",
    "# ========================\n",
    "def main():\n",
    "    base_directory = \"TrainingData\"  # 替换为你的文件夹路径\n",
    "    image_datasets = load_images_to_dict(base_directory)\n",
    "    # 打印结构信息\n",
    "    print(f\"Loaded {len(image_datasets)} folders\")\n",
    "    for folder, images in image_datasets.items():\n",
    "        print(f\"{folder}: {len(images)} images, shape={images[0].shape if images else 'N/A'}\")\n",
    "    \n",
    "    env = SurvivalGameEnv(image_datasets)\n",
    "    \n",
    "    # Pretrain Autoencoder\n",
    "    print(\"Pretraining Autoencoder...\")\n",
    "    autoencoder_model = train_autoencoder(image_datasets, epochs=2, ae_save_path=\"autoencoder.pth\") # Short epochs for test\n",
    "    \n",
    "    # Training parameters\n",
    "    TRAIN_TIMESTEPS = 2000 # Very short for testing, increase for real training (e.g., 50000+)\n",
    "\n",
    "    # Train Fitness model\n",
    "    print(\"\\nTraining Fitness model...\")\n",
    "    fitness_model = fitness_training(env, total_timesteps=TRAIN_TIMESTEPS)\n",
    "    fitness_model.save(\"ppo_fitness_model\")\n",
    "    \n",
    "    # Train Truth model\n",
    "    print(\"\\nTraining Truth model...\")\n",
    "    truth_model = truth_training(env, ae_path=\"autoencoder.pth\", total_timesteps=TRAIN_TIMESTEPS)\n",
    "    truth_model.save(\"ppo_truth_model\")\n",
    "    \n",
    "    # Load models if needed (example)\n",
    "    # fitness_model = PPO.load(\"ppo_fitness_model\", env=env)\n",
    "    # truth_model = PPO.load(\"ppo_truth_model\", env=env)\n",
    "\n",
    "    # Evaluate models\n",
    "    print(\"\\nEvaluating models for survival...\")\n",
    "    # 非实时渲染fitness模型评估\n",
    "    # fitness_score = evaluate_survival(fitness_model, env, n_episodes=5)\n",
    "    # 实时渲染fitness模型评估\n",
    "    fitness_score = evaluate_survival_with_render(fitness_model, env, n_episodes=1)\n",
    "    truth_score = evaluate_survival(truth_model, env, n_episodes=5)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"Fitness Model Average Survival: {fitness_score:.1f} steps\")\n",
    "    print(f\"Truth Model Average Survival: {truth_score:.1f} steps\")\n",
    "\n",
    "    # Truth visualization evaluation\n",
    "    visualize_truth_evaluation(fitness_model, env, run_name=\"FitnessModel\")\n",
    "    visualize_truth_evaluation(truth_model, env, run_name=\"TruthModel\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
